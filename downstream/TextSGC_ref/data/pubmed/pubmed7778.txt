Unlike human medical records most of the veterinary records are free text without standard diagnosis coding. The lack of systematic coding is a major barrier to the growing interest in leveraging veterinary records for public health and translational research. Recent machine learning effort is limited to predicting 42 top-level diagnosis categories from veterinary notes. Here we develop a large-scale algorithm to automatically predict all 4577 standard veterinary diagnosis codes from free text. We train our algorithm on a curated dataset of over 100\u2009K expert labeled veterinary notes and over one million unlabeled notes. Our algorithm is based on the adapted Transformer architecture and we demonstrate that large-scale language modeling on the unlabeled notes via pretraining and as an auxiliary objective during supervised learning greatly improves performance. We systematically evaluate the performance of the model and several baselines in challenging settings where algorithms trained on one hospital are evaluated in a different hospital with substantial domain shift. In addition we show that hierarchical training can address severe data imbalances for fine-grained diagnosis with a few training cases and we provide interpretation for what is learned by the deep network. Our algorithm addresses an important challenge in veterinary medicine and our model and experiments add insights into the power of unsupervised learning for clinical natural language processing. VetTag: improving automated veterinary diagnosis coding via large-scale language modeling.