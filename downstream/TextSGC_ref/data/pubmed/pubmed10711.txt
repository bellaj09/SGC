Omnidirectional and 360 images are becoming widespread in industry and in consumer society causing omnidirectional computer vision to gain attention. Their wide field of view allows the gathering of a great amount of information about the environment from only an image. However the distortion of these images requires the development of specific algorithms for their treatment and interpretation. Moreover a high number of images is essential for the correct training of computer vision algorithms based on learning. In this paper we present a tool for generating datasets of omnidirectional images with semantic and depth information. These images are synthesized from a set of captures that are acquired in a realistic virtual environment for Unreal Engine 4 through an interface plugin. We gather a variety of well-known projection models such as equirectangular and cylindrical panoramas different fish-eye lenses catadioptric systems and empiric models. Furthermore we include in our tool photorealistic non-central-projection systems as non-central panoramas and non-central catadioptric systems. As far as we know this is the first reported tool for generating photorealistic non-central images in the literature. Moreover since the omnidirectional images are made virtually we provide pixel-wise information about semantics and depth as well as perfect knowledge of the calibration parameters of the cameras. This allows the creation of ground-truth information with pixel precision for training learning algorithms and testing 3D vision approaches. To validate the proposed tool different computer vision algorithms are tested as line extractions from dioptric and catadioptric central images 3D Layout recovery and SLAM using equirectangular panoramas and 3D reconstruction from non-central panoramas. OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision.