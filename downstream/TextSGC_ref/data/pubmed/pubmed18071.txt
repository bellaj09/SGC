"Combining multi-modality brain data for disease diagnosis commonly leads to improved performance. A challenge in using multimodality data is that the data are commonly incomplete; namely some modality might be missing for some subjects. In this work we proposed a deep learning based framework for estimating multi-modality imaging data. Our method takes the form of convolutional neural networks where the input and output are two volumetric modalities. The network contains a large number of trainable parameters that capture the relationship between input and output modalities. When trained on subjects with all modalities the network can estimate the output modality given the input modality. We evaluated our method on the Alzheimers Disease Neuroimaging Initiative ADNI database where the input and output modalities are MRI and PET images respectively. Results showed that our method significantly outperformed prior methods." Deep learning based imaging data completion for improved brain disease diagnosis.