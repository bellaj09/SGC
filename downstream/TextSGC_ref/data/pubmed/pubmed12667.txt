Cross-modal hashing has attracted increasing research attention due to its efficiency for large-scale multimedia retrieval. With simultaneous feature representation and hash function learning deep cross-modal hashing DCMH methods have shown superior performance. However most existing methods on DCMH adopt binary quantization functions e.g. Formula: see text to generate hash codes which limit the retrieval performance since binary quantization functions are sensitive to the variations of numeric values. Toward this end we propose a novel end-to-end ranking-based hashing framework in this paper termed as deep semantic-preserving ordinal hashing DSPOH to learn hash functions with deep neural networks by exploring the ranking structure of feature dimensions. In DSPOH the ordinal representation which encodes the relative rank ordering of feature dimensions is explored to generate hash codes. Such ordinal embedding benefits from the numeric stability of rank correlation measures. To make the hash codes discriminative the ordinal representation is expected to well predict the class labels so that the ranking-based hash function learning is optimally compatible with the label predicting. Meanwhile the intermodality similarity is preserved to guarantee that the hash codes of different modalities are consistent. Importantly DSPOH can be effectively integrated with different types of network architectures which demonstrates the flexibility and scalability of our proposed hashing framework. Extensive experiments on three widely used multimodal data sets show that DSPOH outperforms state of the art for cross-modal retrieval tasks. Deep Semantic-Preserving Ordinal Hashing for Cross-Modal Similarity Search.