Visual attention is often predictive for future actions in humans. In manipulation tasks the eyes tend to fixate an object of interest even before the reach-to-grasp is initiated. Some recent studies have proposed to exploit this anticipatory gaze behavior to improve the control of dexterous upper limb prostheses. This requires a detailed understanding of visuomotor coordination to determine in which temporal window gaze may provide helpful information. In this paper we verify and quantify the gaze and motor behavior of 14 transradial amputees who were asked to grasp and manipulate common household objects with their missing limb. For comparison we also include data from 30 able-bodied subjects who executed the same protocol with their right arm. The dataset contains gaze first person video angular velocities of the head and electromyography and accelerometry of the forearm. To analyze the large amount of video we developed a procedure based on recent deep learning methods to automatically detect and segment all objects of interest. This allowed us to accurately determine the pixel distances between the gaze point the target object and the limb in each individual frame. Our analysis shows a clear coordination between the eyes and the limb in the reach-to-grasp phase confirming that both intact and amputated subjects precede the grasp with their eyes by more than 500 ms. Furthermore we note that the gaze behavior of amputees was remarkably similar to that of the able-bodied control group despite their inability to physically manipulate the objects. On the Visuomotor Behavior of Amputees and Able-Bodied People During Grasping.