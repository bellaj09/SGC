Deep neural networks have been recently shown to capture intricate information transformation of signals from the sensory profiles to semantic representations that facilitate recognition or discrimination of complex stimuli. In this vein convolutional neural networks CNNs have been used very successfully in image and audio classification. Designed to imitate the hierarchical structure of the nervous system CNNs reflect activation with increasing degrees of complexity that transform the incoming signal onto object-level representations. In this work we employ a CNN trained for large-scale audio object classification to gain insights about the contribution of various audio representations that guide sound perception. The analysis contrasts activation of different layers of a CNN with acoustic features extracted directly from the scenes perceptual salience obtained from behavioral responses of human listeners as well as neural oscillations recorded by electroencephalography EEG in response to the same natural scenes. All three measures are tightly linked quantities believed to guide percepts of salience and object formation when listening to complex scenes. The results paint a picture of the intricate interplay between low-level and object-level representations in guiding auditory salience that is very much dependent on context and sound category. Connecting Deep Neural Networks to Physical Perceptual and Electrophysiological Auditory Signals.