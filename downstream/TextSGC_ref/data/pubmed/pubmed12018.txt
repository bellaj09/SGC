To evaluate deep learning in the assessment of breast cancer risk in which convolutional neural networks CNNs with transfer learning are used to extract parenchymal characteristics directly from full-field digital mammographic FFDM images instead of using computerized radiographic texture analysis RTA 456 clinical FFDM cases were included: a "high-risk" BRCA1/2 gene-mutation carriers dataset 53 cases a "high-risk" unilateral cancer patients dataset 75 cases and a "low-risk dataset" 328 cases. Deep learning was compared to the use of features from RTA as well as to a combination of both in the task of distinguishing between high- and low-risk subjects. Similar classification performances were obtained using CNN area under the curve Formula: see text; standard error Formula: see text and RTA Formula: see text; Formula: see text in distinguishing BRCA1/2 carriers and low-risk women. However in distinguishing unilateral cancer patients and low-risk women performance was significantly greater with CNN Formula: see text; Formula: see text compared to RTA Formula: see text; Formula: see text. Fusion classifiers performed significantly better than the RTA-alone classifiers with AUC values of 0.86 and 0.84 in differentiating BRCA1/2 carriers from low-risk women and unilateral cancer patients from low-risk women respectively. In conclusion deep learning extracted parenchymal characteristics from FFDMs performed as well as or better than conventional texture analysis in the task of distinguishing between cancer risk populations. Deep learning in breast cancer risk assessment: evaluation of convolutional neural networks on a clinical dataset of full-field digital mammograms.