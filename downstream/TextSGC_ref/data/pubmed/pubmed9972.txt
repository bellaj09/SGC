Nonrigid multimodal image registration remains a challenging task in medical image processing and analysis. The structural representation SR-based registration methods have attracted much attention recently. However the existing SR methods cannot provide satisfactory registration accuracy due to the utilization of hand-designed features for structural representation. To address this problem the structural representation method based on the improved version of the simple deep learning network named PCANet is proposed for medical image registration. In the proposed method PCANet is firstly trained on numerous medical images to learn convolution kernels for this network. Then a pair of input medical images to be registered is processed by the learned PCANet. The features extracted by various layers in the PCANet are fused to produce multilevel features. The structural representation images are constructed for two input images based on nonlinear transformation of these multilevel features. The Euclidean distance between structural representation images is calculated and used as the similarity metrics. The objective function defined by the similarity metrics is optimized by L-BFGS method to obtain parameters of the free-form deformation FFD model. Extensive experiments on simulated and real multimodal image datasets show that compared with the state-of-the-art registration methods such as modality-independent neighborhood descriptor MIND normalized mutual information NMI Weber local descriptor WLD and the sum of squared differences on entropy images ESSD the proposed method provides better registration performance in terms of target registration error TRE and subjective human vision. PCANet-Based Structural Representation for Nonrigid Multimodal Medical Image Registration.