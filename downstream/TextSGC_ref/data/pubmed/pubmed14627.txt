Image transformation between multiple domains has become a challenging problem in deep generative networks. This is because in real-world applications finding paired images in different domains is an expensive and impractical task. This paper proposes a new model named joint moment-matching autoencodersJMA. This model learns to perform cross-domain transformation over multiple domains based on perceptual loss and maximum mean discrepancy criteria in the absence of any paired images between the domains. Our results show that the proposed JMA framework successfully learns to transform images between domains without any paired data. We demonstrate that our model has good performance in the generative context as well as in the domain transformation tasks with better computational efficiency than conventional methods. Joint moment-matching autoencoders.