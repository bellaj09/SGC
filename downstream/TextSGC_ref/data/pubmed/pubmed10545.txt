To achieve effective visual tracking a robust feature representation composed of two separate components i.e. feature learning and selection for an object is one of the key issues. Typically a common assumption used in visual tracking is that the raw video sequences are clear while real-world data is with significant noise and irrelevant patterns. Consequently the learned features may be not all relevant and noisy. To address this problem we propose a novel visual tracking method via a point-wise gated convolutional deep network CPGDN that jointly performs the feature learning and feature selection in a unified framework. The proposed method performs dynamic feature selection on raw features through a gating mechanism. Therefore the proposed method can adaptively focus on the task-relevant patterns i.e. a target object while ignoring the task-irrelevant patterns i.e. the surrounding background of a target object. Specifically inspired by transfer learning we firstly pre-train an object appearance model offline to learn generic image features and then transfer rich feature hierarchies from an offline pre-trained CPGDN into online tracking. In online tracking the pre-trained CPGDN model is fine-tuned to adapt to the tracking specific objects. Finally to alleviate the tracker drifting problem inspired by an observation that a visual target should be an object rather than not we combine an edge box-based object proposal method to further improve the tracking accuracy. Extensive evaluation on the widely used CVPR2013 tracking benchmark validates the robustness and effectiveness of the proposed method. Jointly Feature Learning and Selection for Robust Tracking via a Gating Mechanism.