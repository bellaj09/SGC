"Speech signals are degraded in real-life environments as a product of background noise or other factors. The processing of such signals for voice recognition and voice analysis systems presents important challenges. One of the conditions that make adverse quality difficult to handle in those systems is reverberation produced by sound wave reflections that travel from the source to the microphone in multiple directions. To enhance signals in such adverse conditions several deep learning-based methods have been proposed and proven to be effective. Recently recurrent neural networks especially those with long short-term memory LSTM have presented surprising results in tasks related to time-dependent processing of signals such as speech. One of the most challenging aspects of LSTM networks is the high computational cost of the training procedure which has limited extended experimentation in several cases. In this work we present a proposal to evaluate the hybrid models of neural networks to learn different reverberation conditions without any previous information. The results show that some combinations of LSTM and perceptron layers produce good results in comparison to those from pure LSTM networks given a fixed number of layers. The evaluation was made based on quality measurements of the signals spectrum the training time of the networks and statistical validation of results. In total 120 artificial neural networks of eight different types were trained and compared. The results help to affirm the fact that hybrid networks represent an important solution for speech signal enhancement given that reduction in training time is on the order of 30% in processes that can normally take several days or weeks depending on the amount of data. The results also present advantages in efficiency but without a significant drop in quality." Evaluation of Mixed Deep Neural Networks for Reverberant Speech Enhancement.