We present a physically-motivated topology of a deep neural network that can efficiently infer extensive parameters such as energy entropy or number of particles of arbitrarily large systems doing so with scaling. We use a form of domain decomposition for training and inference where each sub-domain tile is comprised of a non-overlapping focus region surrounded by an overlapping context region. The size of these regions is motivated by the physical interaction length scales of the problem. We demonstrate the application of EDNNs to three physical systems: the Ising model and two hexagonal/graphene-like datasets. In the latter an EDNN was able to make total energy predictions of a 60 atoms system with comparable accuracy to density functional theory DFT in 57 milliseconds. Additionally EDNNs are well suited for massively parallel evaluation as no communication is necessary during neural network evaluation. We demonstrate that EDNNs can be used to make an energy prediction of a two-dimensional 35.2 million atom system over 1.0 m2 of material at an accuracy comparable to DFT in under 25 minutes. Such a system exists on a length scale visible with optical microscopy and larger than some living organisms. Extensive deep neural networks for transferring small scale learning to large scale systems.