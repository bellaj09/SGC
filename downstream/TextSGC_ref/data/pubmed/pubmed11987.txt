Motion models have been proved to be a crucial part in the visual tracking process. In recent trackers particle filter and sliding windows-based motion models have been widely used. Treating motion models as a sequence prediction problem we can estimate the motion of objects using their trajectories. Moreover it is possible to transfer the learned knowledge from annotated trajectories to new objects. Inspired by recent advance in deep learning for visual feature extraction and sequence prediction we propose a trajectory predictor to learn prior knowledge from annotated trajectories and transfer it to predict the motion of target objects. In this predictor convolutional neural networks extract the visual features of target objects. Long short-term memory model leverages the annotated trajectory priors as well as sequential visual information which includes the tracked features and center locations of the target object to predict the motion. Furthermore to extend this method to videos in which it is difficult to obtain annotated trajectories a dynamic weighted motion model that combines the proposed trajectory predictor with a random sampler is proposed. To evaluate the transfer performance of the proposed trajectory predictor we annotated a real-world vehicle dataset. Experiment results on both this real-world vehicle dataset and an online tracker benchmark dataset indicate that the proposed method outperforms several state-of-the-art trackers. Trajectory Predictor by Using Recurrent Neural Networks in Visual Tracking.