A novel adversarial attack methodology for fooling deep neural network classifiers in image classification tasks is proposed along with a novel defense mechanism to counter such attacks. Two concepts are introduced namely the K-Anonymity-inspired Adversarial Attack K-A3 and the Multiple Support Vector Data Description Defense M-SVDD-D. The proposed K-A3 introduces novel optimization criteria to standard adversarial attack methodologies inspired by the K-Anonymity principles. Its generated adversarial examples are not only misclassified by the neural network classifier but are uniformly spread along K different ranked output positions. The proposed M-SVDD-D consists of a deep neural architecture layer consisting of multiple non-linear one-class classifiers based on Support Vector Data Description that can be used to replace the final linear classification layer of a deep neural architecture and an additional class verification mechanism. Its application decreases the effectiveness of adversarial attacks by increasing the noise energy required to deceive the protected model attributed to the introduced non-linearity. In addition M-SVDD-D can be used to prevent adversarial attacks in black-box attack settings. K-Anonymity inspired adversarial attack and multiple one-class classification defense.