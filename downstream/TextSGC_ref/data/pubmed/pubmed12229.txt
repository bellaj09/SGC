"When blind and deaf people are passengers in fully autonomous vehicles an intuitive and accurate visualization screen should be provided for the deaf and an audification system with speech-to-text STT and text-to-speech TTS functions should be provided for the blind. However these systems cannot know the fault self-diagnosis information and the instrument cluster information that indicates the current state of the vehicle when driving. This paper proposes an audification and visualization system AVS of an autonomous vehicle for blind and deaf people based on deep learning to solve this problem. The AVS consists of three modules. The data collection and management module DCMM stores and manages the data collected from the vehicle. The audification conversion module ACM has a speech-to-text submodule STS that recognizes a users speech and converts it to text data and a text-to-wave submodule TWS that converts text data to voice. The data visualization module DVM visualizes the collected sensor data fault self-diagnosis data etc. and places the visualized data according to the size of the vehicles display. The experiment shows that the time taken to adjust visualization graphic components in on-board diagnostics OBD was approximately 2.5 times faster than the time taken in a cloud server. In addition the overall computational time of the AVS system was approximately 2 ms faster than the existing instrument cluster. Therefore because the AVS proposed in this paper can enable blind and deaf people to select only what they want to hear and see it reduces the overload of transmission and greatly increases the safety of the vehicle. If the AVS is introduced in a real vehicle it can prevent accidents for disabled and other passengers in advance." An Audification and Visualization System AVS of an Autonomous Vehicle for Blind and Deaf People Based on Deep Learning.