"Recent work in computer science has shown the power of deep learning driven by the backpropagation algorithm in networks of artificial neurons. But real neurons in the brain are different from most of these artificial ones in at least three crucial ways: they emit spikes rather than graded outputs their inputs and outputs are related dynamically rather than by piecewise-smooth functions and they have no known way to coordinate arrays of synapses in separate forward and feedback pathways so that they change simultaneously and identically as they do in backpropagation. Given these differences it is unlikely that current deep learning algorithms can operate in the brain but we that show these problems can be solved by two simple devices: learning rules can approximate dynamic input-output relations with piecewise-smooth functions and a variation on the feedback alignment algorithm can train deep networks without having to coordinate forward and feedback synapses. Our results also show that deep spiking networks learn much better if each neuron computes an intracellular teaching signal that reflects that cells nonlinearity. With this mechanism networks of spiking neurons show useful learning in synapses at least nine layers upstream from the output cells and perform well compared to other spiking networks in the literature on the MNIST digit recognition task." Deep Learning with Dynamic Spiking Neurons and Fixed Feedback Weights.