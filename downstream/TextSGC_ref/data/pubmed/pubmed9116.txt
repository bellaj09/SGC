Learning depth from a single image as an important issue in scene understanding has attracted a lot of attention in the past decade. The accuracy of the depth estimation has been improved from conditional Markov random fields non-parametric methods to deep convolutional neural networks most recently. However there exist inherent ambiguities in recovering 3D from a single 2D image. In this paper we first prove the ambiguity between the focal length and monocular depth learning and verify the result using experiments showing that the focal length has a great influence on accurate depth recovery. In order to learn monocular depth by embedding the focal length we propose a method to generate synthetic varying-focal-length dataset from fixed-focal-length datasets and a simple and effective method is implemented to fill the holes in the newly generated images. For the sake of accurate depth recovery we propose a novel deep neural network to infer depth through effectively fusing the middle-level information on the fixed-focal-length dataset which outperforms the state-of-the-art methods built on pretrained VGG. Furthermore the newly generated varying-focallength dataset is taken as input to the proposed network in both learning and inference phases. Extensive experiments on the fixed- and varying-focal-length datasets demonstrate that the learned monocular depth with embedded focal length is significantly improved compared to that without embedding the focal length information. Learning Depth from Single Images with Deep Neural Network Embedding Focal Length.