Pain is an unpleasant feeling that has been shown to be an important factor for the recovery of patients. Since this is costly in human resources and difficult to do objectively there is the need for automatic systems to measure it. In this paper contrary to current state-of-the-art techniques in pain assessment which are based on facial features only we suggest that the performance can be enhanced by feeding the raw frames to deep learning models outperforming the latest state-of-the-art results while also directly facing the problem of imbalanced data. As a baseline our approach first uses convolutional neural networks CNNs to learn facial features from VGG_Faces which are then linked to a long short-term memory to exploit the temporal relation between video frames. We further compare the performances of using the so popular schema based on the canonically normalized appearance versus taking into account the whole image. As a result we outperform current state-of-the-art area under the curve performance in the UNBC-McMaster Shoulder Pain Expression Archive Database. In addition to evaluate the generalization properties of our proposed methodology on facial motion recognition we also report competitive results in the Cohn Kanade+ facial expression database. Deep Pain: Exploiting Long Short-Term Memory Networks for Facial Expression Classification.