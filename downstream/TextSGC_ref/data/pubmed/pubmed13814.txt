For deep learning based speech segregation to have translational significance as a noise-reduction tool it must perform in a wide variety of acoustic environments. In the current study performance was examined when target speech was subjected to interference from a single talker and room reverberation. Conditions were compared in which an algorithm was trained to remove both reverberation and interfering speech or only interfering speech. A recurrent neural network incorporating bidirectional long short-term memory was trained to estimate the ideal ratio mask corresponding to target speech. Substantial intelligibility improvements were found for hearing-impaired HI and normal-hearing NH listeners across a range of target-to-interferer ratios TIRs. HI listeners performed better with reverberation removed whereas NH listeners demonstrated no difference. Algorithm benefit averaged 56 percentage points for the HI listeners at the least-favorable TIR allowing these listeners to perform numerically better than young NH listeners without processing. The current study highlights the difficulty associated with perceiving speech in reverberant-noisy environments and it extends the range of environments in which deep learning based speech segregation can be effectively applied. This increasingly wide array of environments includes not only a variety of background noises and interfering speech but also room reverberation. A deep learning algorithm to increase intelligibility for hearing-impaired listeners in the presence of a competing talker and reverberation.