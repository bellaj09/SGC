Increasingly large deep learning architectures such as Deep Belief Networks DBNs are the focus of current machine learning research and achieve state-of-the-art results in different domains. However both training and execution of large-scale Deep Networks require vast computing resources leading to high power requirements and communication overheads. The on-going work on design and construction of spike-based hardware platforms offers an alternative for running deep neural networks with significantly lower power consumption but has to overcome hardware limitations in terms of noise and limited weight precision as well as noise inherent in the sensor signal. This article investigates how such hardware constraints impact the performance of spiking neural network implementations of DBNs. In particular the influence of limited bit precision during execution and training and the impact of silicon mismatch in the synaptic weight parameters of custom hybrid VLSI implementations is studied. Furthermore the network performance of spiking DBNs is characterized with regard to noise in the spiking input signal. Our results demonstrate that spiking DBNs can tolerate very low levels of hardware bit precision down to almost two bits and show that their performance can be improved by at least 30% through an adapted training mechanism that takes the bit precision of the target platform into account. Spiking DBNs thus present an important use-case for large-scale hybrid analog-digital or digital neuromorphic platforms such as SpiNNaker which can execute large but precision-constrained deep networks in real time. Robustness of spiking Deep Belief Networks to noise and reduced bit precision of neuro-inspired hardware platforms.