Deep stacking networks DSNs have been successfully applied in classification tasks. Its architecture builds upon blocks of simplified neural network modules SNNM. The hidden units are assumed to be independent in the SNNM module. However this assumption prevents SNNM from learning the local dependencies between hidden units to better capture the information in the input data for the classification task. In addition the hidden representations of input data in each class can be expectantly split into a group in real-world classification applications. Therefore we propose two kinds of group sparse SNNM modules by mixing -norm and -norm. The first module learns the local dependencies among hidden units by dividing them into non-overlapping groups. The second module splits the representations of samples in different classes into separate groups to cluster the samples in each class. A group sparse DSN GS-DSN is constructed by stacking the group sparse SNNM modules. Experimental results further verify that our GS-DSN model outperforms the relevant classification methods. Particularly GS-DSN achieves the state-of-the-art performance 99.1% on 15-Scene. Visual Representation and Classification by Learning Group Sparse Deep Stacking Network.