In qualitative or quantitative studies of structure-activity relationships SARs machine learning ML models are trained to recognize structural patterns that differentiate between active and inactive compounds. Understanding model decisions is challenging but of critical importance to guide compound design. Moreover the interpretation of ML results provides an additional level of model validation based on expert knowledge. A number of complex ML approaches especially deep learning DL architectures have distinctive black-box character. Herein a locally interpretable explanatory method termed Shapley additive explanations SHAP is introduced for rationalizing activity predictions of any ML algorithm regardless of its complexity. Models resulting from random forest RF nonlinear support vector machine SVM and deep neural network DNN learning are interpreted and structural patterns determining the predicted probability of activity are identified and mapped onto test compounds. The results indicate that SHAP has high potential for rationalizing predictions of complex ML models. Interpretation of Compound Activity Predictions from Complex Machine Learning Models Using Local Approximations and Shapley Values.