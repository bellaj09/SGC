In this paper a novel benchmark is introduced for evaluating local image descriptors. We demonstrate limitations of the commonly used datasets and evaluation protocols that lead to ambiguities and contradictory results in the literature. Furthermore these benchmarks are nearly saturated due to the recent improvements in local descriptors obtained by learning from large annotated datasets. To address these issues we introduce a new large dataset suitable for training and testing modern descriptors together with strictly defined evaluation protocols in several tasks such as matching retrieval and verification. This allows for more realistic thus more reliable comparisons in different application scenarios. We evaluate the performance of several state-of-the-art descriptors and analyse their properties. We show that a simple normalisation of traditional hand-crafted descriptors is able to boost their performance to the level of deep learning based descriptors once realistic benchmarks are considered. Additionally we specify a protocol for learning and evaluating using cross validation. We show that when training state-of-the-art descriptors on this dataset the traditional verification task is almost entirely saturated. HPatches: A benchmark and evaluation of handcrafted and learned local descriptors.