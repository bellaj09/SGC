Person re-identification re-ID favors discriminative representations over unseen shots to recognize identities in disjoint camera views. Effective methods are developed via pair-wise similarity learning to detect a fixed set of region features which can be mapped to compute the similarity value. However relevant parts of each image are detected independently without referring to the correlation on the other image. Also region-based methods spatially position local features for their aligned similarities. In this article we introduce the deep coattention-based comparator DCC to fuse codependent representations of paired images so as to correlate the best relevant parts and produce their relative representations accordingly. The proposed approach mimics the human foveation to detect the distinct regions concurrently across images and alternatively attends to fuse them into the similarity learning. Our comparator is capable of learning representations relative to a test shot and well-suited to reidentifying pedestrians in surveillance. We perform extensive experiments to provide the insights and demonstrate the state of the arts achieved by our method in benchmark data sets: 1.2 and 2.5 points gain in mean average precision mAP on DukeMTMC-reID and Market-1501 respectively. Deep Coattention-Based Comparator for Relative Representation Learning in Person Re-Identification.