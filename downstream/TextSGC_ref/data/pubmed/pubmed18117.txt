"For the last decade it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimers Disease AD and its prodromal stage Mild Cognitive Impairment MCI and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging MRI and Positron Emission Tomography PET. To our best knowledge the previous methods in the literature mostly used hand-crafted features such as cortical thickness gray matter densities from MRI or voxel intensities from PET and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically we use Deep Boltzmann Machine DBM2 a deep network with a restricted Boltzmann machine as a building block to find a latent hierarchical feature representation from a 3D patch and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control NC MCI vs. NC and MCI converter vs. MCI non-converter we obtained the maximal accuracies of 95.35% 85.67% and 74.58% respectively outperforming the competing methods. By visual inspection of the trained model we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET." Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis.