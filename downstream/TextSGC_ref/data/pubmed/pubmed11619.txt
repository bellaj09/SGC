Learning of hierarchical features with spiking neurons has mostly been investigated in the database framework of standard deep learning systems. However the properties of neuromorphic systems could be particularly interesting for learning from continuous sensor data in real-world settings. In this work we introduce a deep spiking convolutional neural network of integrate-and-fire IF neurons which performs unsupervised online deep learning with spike-timing dependent plasticity STDP from a stream of asynchronous and continuous event-based data. In contrast to previous approaches to unsupervised deep learning with spikes where layers were trained successively we introduce a mechanism to train all layers of the network simultaneously. This allows approximate online inference already during the learning process and makes our architecture suitable for online learning and inference. We show that it is possible to train the network without providing implicit information about the database such as the number of classes and the duration of stimuli presentation. By designing an STDP learning rule which depends only on relative spike timings we make our network fully event-driven and able to operate without defining an absolute timescale of its dynamics. Our architecture requires only a small number of generic mechanisms and therefore enforces few constraints on a possible neuromorphic hardware implementation. These characteristics make our network one of the few neuromorphic architecture which could directly learn features and perform inference from an event-based vision sensor. Event-Based Timescale Invariant Unsupervised Online Deep Learning With STDP.