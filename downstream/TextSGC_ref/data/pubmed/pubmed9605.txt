radient descent optimization of learning has become a paradigm for training deep convolutional neural networks DCNN. However utilizing other learning strategies in the training process of the DCNN has rarely been explored by the deep learning DL community. This serves as the motivation to introduce a non-iterative learning strategy to retrain neurons at the top dense or fully connected FC layers of DCNN resulting in higher performance.radient descent optimization of learning has become a paradigm for training deep convolutional neural networks DCNN. However utilizing other learning strategies in the training process of the DCNN has rarely been explored by the deep learning DL community. This serves as the motivation to introduce a non-iterative learning strategy to retrain neurons at the top dense or fully connected FC layers of DCNN resulting in higher performance.G The proposed method exploits the Moore-Penrose Inverse to pull back the current residual error to each FC layer generating well-generalized features. Further the weights of each FC layers are recomputed according to the Moore-Penrose Inverse. We evaluate the proposed approach on six most widely accepted object recognition benchmark datasets: Scene- 15 CIFAR-10 CIFAR-100 SUN-397 Places365 and ImageNet. The experimental results show that the proposed method obtains improvements over 30 state-of-the-art methods. Interestingly it also indicates that any DCNN with the proposed method can provide better performance than the same network with its original Backpropagation BP- based training. Recomputation of dense layers for the performance improvement of DCNN.