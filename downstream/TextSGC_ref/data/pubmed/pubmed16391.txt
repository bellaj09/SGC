Although an action observation network and mirror neurons for understanding the actions and intentions of others have been under deep interdisciplinary consideration over recent years it remains largely unknown how the brain manages to map visually perceived biological motion of others onto its own motor system. This paper shows how such a mapping may be established even if the biologically motion is visually perceived from a new vantage point. We introduce a learning artificial neural network model and evaluate it on full body motion tracking recordings. The model implements an embodied predictive inference approach. It first learns to correlate and segment multimodal sensory streams of own bodily motion. In doing so it becomes able to anticipate motion progression to complete missing modal information and to self-generate learned motion sequences. When biological motion of another person is observed this self-knowledge is utilized to recognize similar motion patterns and predict their progress. Due to the relative encodings the model shows strong robustness in recognition despite observing rather large varieties of body morphology and posture dynamics. By additionally equipping the model with the capability to rotate its visual frame of reference it is able to deduce the visual perspective onto the observed person establishing full consistency to the embodied self-motion encodings by means of active inference. In further support of its neuro-cognitive plausibility we also model typical bistable perceptions when crucial depth information is missing. In sum the introduced neural model proposes a solution to the problem of how the human brain may establish correspondence between observed bodily motion and its own motor system thus offering a mechanism that supports the development of mirror neurons. Embodied learning of a generative neural model for biological motion perception and inference.