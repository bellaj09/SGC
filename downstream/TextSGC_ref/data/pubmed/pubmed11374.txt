Head pose estimation is a fundamental task for face and social related research. Although 3D morphable model 3DMM based methods relying on depth information usually achieve accurate results they usually require frontal or mid-profile poses which preclude a large set of applications where such conditions can not be garanteed like monitoring natural interactions from fixed sensors placed in the environment. A major reason is that 3DMM models usually only cover the face region. In this paper we present a framework which combines the strengths of a 3DMM model fitted online with a prior-free reconstruction of a 3D full head model providing support for pose estimation from any viewpoint. In addition we also proposes a symmetry regularizer for accurate 3DMM fitting under partial observations and exploit visual tracking to address natural head dynamics with fast accelerations. Extensive experiments show that our method achieves state-of-the-art performance on the public BIWI dataset as well as accurate and robust results on UbiPose an annotated dataset of natural interactions that we make public and where adverse poses occlusions or fast motions regularly occur. HeadFusion: 360 Head Pose Tracking Combining 3D Morphable Model and 3D Reconstruction.