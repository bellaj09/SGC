Using multi-atlas registration MAR information carried by atlases can be transferred onto a new input image for the tasks of region of interest ROI segmentation anatomical landmark detection and so on. Conventional atlases used in MAR methods are monomodal and contain only normal anatomical structures. Therefore the majority of MAR methods cannot handle input multimodal pathological images which are often collected in routine image-based diagnosis. This is because registering monomodal atlases with normal appearances to multimodal pathological images involves two major problems: 1 missing imaging modalities in the monomodal atlases and 2 influence from pathological regions. In this paper we propose a new MAR framework to tackle these problems. In this framework a deep learning based image synthesizers are applied for synthesizing multimodal normal atlases from conventional monomodal normal atlases. To reduce the influence from pathological regions we further propose a multimodal lowrank approach to recover multimodal normal-looking images from multimodal pathological images. Finally the multimodal normal atlases can be registered to the recovered multimodal images in a multi-channel way. We evaluate our MAR framework via brain ROI segmentation of multimodal tumor brain images. Due to the utilization of multimodal information and the reduced influence from pathological regions experimental results show that registration based on our method is more accurate and robust leading to significantly improved brain ROI segmentation compared with state-of-the-art methods. A New Multi-Atlas Registration Framework for Multimodal Pathological Images Using Conventional Monomodal Normal Atlases.