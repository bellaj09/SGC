Robotic and virtual-reality systems offer tremendous potential for improving assessment and rehabilitation of neurological disorders affecting the upper extremity. A key feature of these systems is that visual stimuli are often presented within the same workspace as the hands i.e. peripersonal space. Integrating video-based remote eye tracking with robotic and virtual-reality systems can provide an additional tool for investigating how cognitive processes influence visuomotor learning and rehabilitation of the upper extremity. However remote eye tracking systems typically compute ocular kinematics by assuming eye movements are made in a plane with constant depth e.g. frontal plane. When visual stimuli are presented at variable depths e.g. transverse plane eye movements have a vergence component that may influence reliable detection of gaze events fixations smooth pursuits and saccades. To our knowledge there are no available methods to classify gaze events in the transverse plane for monocular remote eye tracking systems. Here we present a geometrical method to compute ocular kinematics from a monocular remote eye tracking system when visual stimuli are presented in the transverse plane. We then use the obtained kinematics to compute velocity-based thresholds that allow us to accurately identify onsets and offsets of fixations saccades and smooth pursuits. Finally we validate our algorithm by comparing the gaze events computed by the algorithm with those obtained from the eye-tracking software and manual digitization. A geometric method for computing ocular kinematics and classifying gaze events using monocular remote eye tracking in a robotic environment.