Deep learning approaches have achieved breakthrough performance in various domains. However the segmentation of raw eye-movement data into discrete events is still done predominantly either by hand or by algorithms that use hand-picked parameters and thresholds. We propose and make publicly available a small 1D-CNN in conjunction with a bidirectional long short-term memory network that classifies gaze samples as fixations saccades smooth pursuit or noise simultaneously assigning labels in windows of up to 1 s. In addition to unprocessed gaze coordinates our approach uses different combinations of the speed of gaze its direction and acceleration all computed at different temporal scales as input features. Its performance was evaluated on a large-scale hand-labeled ground truth data set GazeCom and against 12 reference algorithms. Furthermore we introduced a novel pipeline and metric for event detection in eye-tracking recordings which enforce stricter criteria on the algorithmically produced events in order to consider them as potentially correct detections. Results show that our deep approach outperforms all others including the state-of-the-art multi-observer smooth pursuit detector. We additionally test our best model on an independent set of recordings where our approach stays highly competitive compared to literature methods. 1D CNN with BLSTM for automated classification of fixations saccades and smooth pursuits.