State-of-the-art methods to infer dense and accurate depth measurements from images rely on deep CNN models trained in an end-to-end fashion on a significant amount of data. However despite the outstanding performance achieved these frameworks suffer a drastic drop in accuracy when dealing with unseen environments much different concerning appearance e.g. synthetic vs. real or context e.g. indoor vs. outdoor from those observed during the training phase. Such domain shift issue is usually softened by fine-tuning on smaller sets of images with depth labels acquired in the target domain with active sensors e.g. LiDAR. However relying on such supervised labeled data is seldom feasible in practical applications. Therefore we propose an effective unsupervised domain adaptation technique enabling to overcome the domain shift problem without requiring any groundtruth label. Our method deploying much more accessible to obtain stereo pairs leverages traditional and not learning-based stereo algorithms to produce disparity/depth labels and on confidence measures to assess their degree of reliability. With these cues we can fine-tune deep models through a novel confidence-guided loss function neglecting the effect of outliers gathered from the output of conventional stereo algorithms. Unsupervised Domain Adaptation for Depth Prediction from Images.