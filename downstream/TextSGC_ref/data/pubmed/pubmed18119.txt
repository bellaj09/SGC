A key problem in spoken language identification LID is to design effective representations which are specific to language information. For example in recent years representations based on both phonotactic and acoustic features have proven their effectiveness for LID. Although advances in machine learning have led to significant improvements LID performance is still lacking especially for short duration speech utterances. With the hypothesis that language information is weak and represented only latently in speech and is largely dependent on the statistical properties of the speech content existing representations may be insufficient. Furthermore they may be susceptible to the variations caused by different speakers specific content of the speech segments and background noise. To address this we propose using Deep Bottleneck Features DBF for spoken LID motivated by the success of Deep Neural Networks DNN in speech recognition. We show that DBFs can form a low-dimensional compact representation of the original inputs with a powerful descriptive and discriminative capability. To evaluate the effectiveness of this we design two acoustic models termed DBF-TV and parallel DBF-TV PDBF-TV using a DBF based i-vector representation for each speech utterance. Results on NIST language recognition evaluation 2009 LRE09 show significant improvements over state-of-the-art systems. By fusing the output of phonotactic and acoustic approaches we achieve an EER of 1.08% 1.89% and 7.01% for 30 s 10 s and 3 s test utterances respectively. Furthermore various DBF configurations have been extensively evaluated and an optimal system proposed. Deep bottleneck features for spoken language identification.