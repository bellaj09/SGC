Recently egocentric activity recognition has attracted considerable attention in the pattern recognition and artificial intelligence communities because of its wide applicability in medical care smart homes and security monitoring. In this study we developed and implemented a deep-learning-based hierarchical fusion framework for the recognition of egocentric activities of daily living ADLs in a wearable hybrid sensor system comprising motion sensors and cameras. Long short-term memory LSTM and a convolutional neural network are used to perform egocentric ADL recognition based on motion sensor data and photo streaming in different layers respectively. The motion sensor data are used solely for activity classification according to motion state while the photo stream is used for further specific activity recognition in the motion state groups. Thus both motion sensor data and photo stream work in their most suitable classification mode to significantly reduce the negative influence of sensor differences on the fusion results. Experimental results show that the proposed method not only is more accurate than the existing direct fusion method by up to 6% but also avoids the time-consuming computation of optical flow in the existing method which makes the proposed algorithm less complex and more suitable for practical application. A Hierarchical Deep Fusion Framework for Egocentric Activity Recognition Using a Wearable Hybrid Sensor System.