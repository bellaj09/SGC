"In this paper we propose a novel deep sparse coding network SCN capable of efficiently adapting its own regularization parameters for a given application. The network is trained end-to-end with a supervised task-driven learning algorithm via error backpropagation. During training the network learns both the dictionaries and the regularization parameters of each sparse coding layer so that the reconstructive dictionaries are smoothly transformed into increasingly discriminative representations. In addition the adaptive regularization also offers the network more flexibility to adjust sparsity levels. Furthermore we have devised a sparse coding layer utilizing a skinny dictionary. Integral to computational efficiency these skinny dictionaries compress the high dimensional sparse codes into lower dimensional structures. The adaptivity and discriminability of our fifteen-layer sparse coding network are demonstrated on five benchmark datasets namely Cifar-10 Cifar-100 STL-10 SVHN and MNIST most of which are considered difficult for sparse coding models. Experimental results show that our architecture overwhelmingly outperforms traditional one-layer sparse coding architectures while using much fewer parameters. Moreover our multilayer architecture exploits the benefits of depth with sparse codings characteristic ability to operate on smaller datasets. In such data-constrained scenarios our technique demonstrates highly competitive performance compared to the deep neural networks." Supervised Deep Sparse Coding Networks for Image Classification.