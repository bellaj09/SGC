Although convolutional neural networks CNNs have been applied to a variety of computational genomics problems there remains a large gap in our understanding of how they build representations of regulatory genomic sequences. Here we perform systematic experiments on synthetic sequences to reveal how CNN architecture specifically convolutional filter size and max-pooling influences the extent that sequence motif representations are learned by first layer filters. We find that CNNs designed to foster hierarchical representation learning of sequence motifs-assembling partial features into whole features in deeper layers-tend to learn distributed representations i.e. partial motifs. On the other hand CNNs that are designed to limit the ability to hierarchically build sequence motif representations in deeper layers tend to learn more interpretable localist representations i.e. whole motifs. We then validate that this representation learning principle established from synthetic sequences generalizes to in vivo sequences. Representation learning of genomic sequence motifs with convolutional neural networks.