Semantic parsing of anatomical structures in X-ray images is a critical task in many clinical applications. Modern methods leverage deep convolutional networks and generally require a large amount of labeled data for model training. However obtaining accurate pixel-wise labels on X-ray images is very challenging due to the appearance of anatomy overlaps and complex texture patterns. In comparison labeled CT data are more accessible since organs in 3D CT scans preserve clearer structures and thus can be easily delineated. In this paper we propose a model framework for learning automatic X-ray image parsing from labeled 3D CT scans. Specifically a Deep Image-to-Image network DI2I for multi-organ segmentation is first trained on X-ray like Digitally Reconstructed Radiographs DRRs rendered from 3D CT volumes. Then we build a Task Driven Generative Adversarial Network TD-GAN to achieve simultaneous synthesis and parsing for unseen real X-ray images. The entire model pipeline does not require any annotations from the X-ray image domain. In the numerical experiments we validate the proposed model on over 800 DRRs and 300 topograms. While the vanilla DI2I trained on DRRs without any adaptation fails completely on segmenting the topograms the proposed model does not require any topogram labels and is able to provide a promising average dice of 86% which achieves the same level of accuracy as results from supervised training 89%. Furthermore we also demonstrate the generality of TD-GAN through quantatitive and qualitative study on widely used public dataset. Unsupervised X-ray image segmentation with task driven generative adversarial networks.