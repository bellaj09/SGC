Although the number of artificial neural network and machine learning architectures is growing at an exponential pace more attention needs to be paid to theoretical guarantees of asymptotic convergence for novel nonlinear high-dimensional adaptive learning algorithms. When properly understood such guarantees can guide the algorithm development and evaluation process and provide theoretical validation for a particular algorithm design. For many decades the machine learning community has widely recognized the importance of stochastic approximation theory as a powerful tool for identifying explicit convergence conditions for adaptive learning machines. However the verification of such conditions is challenging for multidisciplinary researchers not working in the area of stochastic approximation theory. For this reason this letter presents a new stochastic approximation theorem for both passive and reactive learning environments with assumptions that are easily verifiable. The theorem is widely applicable to the analysis and design of important machine learning algorithms including deep learning algorithms with multiple strict local minimizers Monte Carlo expectation-maximization algorithms contrastive divergence learning in Markov fields and policy gradient reinforcement learning. Adaptive Learning Algorithm Convergence in Passive and Reactive Environments.