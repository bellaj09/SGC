There are two key components that can be leveraged for visual tracking: a object appearances; and b object motions. Many existing techniques have recently employed deep learning to enhance visual tracking due to its superior representation power and strong learning ability where most of them employed object appearances but few of them exploited object motions. In this work a deep spatial and temporal network DSTN is developed for visual tracking by explicitly exploiting both the object representations from each frame and their dynamics along multiple frames in a video such that it can seamlessly integrate the object appearances with their motions to produce compact object appearances and capture their temporal variations effectively. Our DSTN method which is deployed into a tracking pipeline in a coarse-to-fine form can perceive the subtle differences on spatial and temporal variations of the target object being tracked and thus it benefits from both off-line training and online fine-tuning. We have also conducted our experiments over four largest tracking benchmarks including OTB-2013 OTB-2015 VOT2015 and VOT2017 and our experimental results have demonstrated that our DSTN method can achieve competitive performance as compared with the state-of-the-art techniques. The source code trained models and all the experimental results of this work will be made public available to facilitate further studies on this problem. Deep Spatial and Temporal Network for Robust Visual Object Tracking.