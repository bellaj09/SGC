Learning a powerful representation for a class with few labeled samples is a challenging problem. Although some state-of-the-art few-shot learning algorithms perform well based on meta-learning they only focus on novel network architecture and fail to take advantage of the knowledge of every classification task. In this paper to accomplish this goal it proposes to combine the channel attention and spatial attention module C-SAM the C-SAM can mine deeply more effective information using samples of different classes that exist in different tasks. The residual network is used to alleviate the loss of the underlying semantic information when the network is deeper. Finally a relation network including a C-SAM is applied to act as a classifier which avoids learning more redundant information and compares the relation between difference samples. The experiment was carried out using the proposed method on six datasets such as miniimagenet Omniglot Caltech-UCSD Birds describable textures dataset Stanford Dogs and Stanford Cars. The experimental results show that the C-SAM outperforms many state-of-the-art few-shot classification methods. Channel-spatial attention network for fewshot classification.