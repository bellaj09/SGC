Existing deep neural networks DNNs are computationally expensive and memory intensive which hinder their further deployment in novel nanoscale devices and applications with lower memory resources or strict latency requirements. In this paper a novel approach to accelerate on-chip learning systems using memristive quantized neural networks M-QNNs is presented. A real problem of multilevel memristive synaptic weights due to device-to-device D2D and cycle-to-cycle C2C variations is considered. Different levels of Gaussian noise are added to the memristive model during each adjustment. Another method of using memristors with binary states to build M-QNNs is presented which suffers from fewer D2D and C2C variations compared with using multilevel memristors. Furthermore methods of solving the sneak path issues in the memristive crossbar arrays are proposed. The M-QNN approach is evaluated on two image classification datasets that is ten-digit number and handwritten images of mixed National Institute of Standards and Technology MNIST. In addition input images with different levels of zero-mean Gaussian noise are tested to verify the robustness of the proposed method. Another highlight of the proposed method is that it can significantly reduce computational time and memory during the process of image recognition. Memristive Quantized Neural Networks: A Novel Approach to Accelerate Deep Learning On-Chip.