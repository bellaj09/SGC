We report on classification of phones and phonetic features from facial electromyographic EMG data within the context of our EMG-based Silent Speech interface. In this paper we show that a Deep Neural Network can be used to perform this classification task yielding a significant improvement over conventional Gaussian Mixture models. Our central contribution is the visualization of patterns which are learned by the neural network. With increasing network depth these patterns represent more and more intricate electromyographic activity. Pattern learning with deep neural networks in EMG-based speech recognition.