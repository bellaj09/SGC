Image representation has been intensively explored in the domain of computer vision for its significant influence on the relative tasks such as image clustering and classification. It is valuable to learn a low-dimensional representation of an image which preserves its inherent information from the original image space. At the perspective of manifold learning this is implemented with the local invariant idea to capture the intrinsic low-dimensional manifold embedded in the high-dimensional input space. Inspired by the recent successes of deep architectures we propose a local invariant deep nonlinear mapping algorithm called graph regularized auto-encoder GAE. With the graph regularization the proposed method preserves the local connectivity from the original image space to the representation space while the stacked auto-encoders provide explicit encoding model for fast inference and powerful expressive capacity for complex modeling. Theoretical analysis shows that the graph regularizer penalizes the weighted Frobenius norm of the Jacobian matrix of the encoder mapping where the weight matrix captures the local property in the input space. Furthermore the underlying effects on the hidden representation space are revealed providing insightful explanation to the advantage of the proposed method. Finally the experimental results on both clustering and classification tasks demonstrate the effectiveness of our GAE as well as the correctness of the proposed theoretical analysis and it also suggests that GAE is a superior solution to the current deep representation learning techniques comparing with variant auto-encoders and existing local invariant methods. Graph Regularized Auto-Encoders for Image Representation.