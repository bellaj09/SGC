We usually look at an object when we are going to manipulate it. Thus eye tracking can be used to communicate intended actions. An effective human-machine interface however should be able to differentiate intentional and spontaneous eye movements. We report an electroencephalogram EEG marker that differentiates gaze fixations used for control from spontaneous fixations involved in visual exploration. Eight healthy participants played a game with their eye movements only. Their gaze-synchronized EEG data fixation-related potentials FRPs were collected during game\s control-on and control-off conditions. A slow negative wave with a maximum in the parietooccipital region was present in each participant\s averaged FRPs in the control-on conditions and was absent or had much lower amplitude in the control-off condition. This wave was similar but not identical to stimulus-preceding negativity a slow negative wave that can be observed during feedback expectation. Classification of intentional vs. spontaneous fixations was based on amplitude features from 13 EEG channels using 300 ms length segments free from electrooculogram contamination 200-500 ms relative to the fixation onset. For the first fixations in the fixation triplets required to make moves in the game classified against control-off data a committee of greedy classifiers provided 0.90  0.07 specificity and 0.38  0.14 sensitivity. Similar slightly lower results were obtained for the shrinkage Linear Discriminate Analysis LDA classifier. The second and third fixations in the triplets were classified at lower rate. We expect that with improved feature sets and classifiers a hybrid dwell-based Eye-Brain-Computer Interface EBCI can be built using the FRP difference between the intended and spontaneous fixations. If this direction of BCI development will be successful such a multimodal interface may improve the fluency of interaction and can possibly become the basis for a new input device for paralyzed and healthy users the EBCI "Wish Mouse." EEG Negativity in Fixations Used for Gaze-Based Control: Toward Converting Intentions into Actions with an Eye-Brain-Computer Interface.