Recent advances in deep learning methods have redefined the state-of-the-art for many medical imaging applications surpassing previous approaches and sometimes even competing with human judgment in several tasks. Those models however when trained to reduce the empirical risk on a single domain fail to generalize when applied to other domains a very common scenario in medical imaging due to the variability of images and anatomical structures even across the same imaging modality. In this work we extend the method of unsupervised domain adaptation using self-ensembling for the semantic segmentation task and explore multiple facets of the method on a small and realistic publicly-available magnetic resonance MRI dataset. Through an extensive evaluation we show that self-ensembling can indeed improve the generalization of the models even when using a small amount of unlabeled data. Unsupervised domain adaptation for medical imaging segmentation with self-ensembling.