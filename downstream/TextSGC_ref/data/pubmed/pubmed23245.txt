One of the fundamental requirements for successful navigation through an environment is the continuous monitoring of distance travelled. To do so humans normally use one or a combination of visual proprioceptive/efferent vestibular and temporal cues. In the real world information from one sensory modality is normally congruent with information from other modalities; hence studying the nature of sensory interactions is often difficult. In order to decouple the natural covariation between different sensory cues we used virtual reality technology to vary the relation between the information generated from visual sources and the information generated from proprioceptive/efferent sources. When we manipulated the stimuli such that the visual information was coupled in various ways to the proprioceptive/efferent information human subjects predominantly used visual information to estimate the ratio of two traversed path lengths. Although proprioceptive/efferent information was not used directly the mere availability of proprioceptive information increased the accuracy of relative path length estimation based on visual cues even though the proprioceptive/efferent information was inconsistent with the visual information. These results convincingly demonstrated that active movement locomotion facilitates visual perception of path length travelled. Multisensory integration in the estimation of relative path length.