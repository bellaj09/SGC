In recent years in an attempt to maximize performance machine learning approaches for event-related potential ERP spelling have become more and more complex. In this paper we have taken a step back as we wanted to improve the performance without building an overly complex model that cannot be used by the community. Our research resulted in a unified probabilistic model for ERP spelling which is based on only three assumptions and incorporates language information. On top of that the probabilistic nature of our classifier yields a natural dynamic stopping strategy. Furthermore our method uses the same parameters across 25 subjects from three different datasets. We show that our classifier when enhanced with language models and dynamic stopping improves the spelling speed and accuracy drastically. Additionally we would like to point out that as our model is entirely probabilistic it can easily be used as the foundation for complex systems in future work. All our experiments are executed on publicly available datasets to allow for future comparison with similar techniques. A unified probabilistic approach to improve spelling in an event-related potential-based brain-computer interface.