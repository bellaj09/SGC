We study active object tracking where a tracker takes visual observations i.e. frame sequences as inputs and produces the corresponding camera control signals as outputs e.g. move forward turn left etc.. Conventional methods tackle tracking and camera control tasks separately and the resulting system is difficult to tune jointly. Such an approach also requires significant human efforts for image labeling and expensive trial-and-error system tuning in real-world. To address these issues we propose in this paper an end-to-end solution via deep reinforcement learning. A ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose environment augmentation techniques and a customized reward function which are crucial for successful training. The tracker trained in simulators ViZDoom Unreal Engine demonstrates good generalization behaviors in the case of unseen object moving paths unseen object appearances unseen backgrounds and distracting objects. The system is robust and can restore tracking after occasional lost of the target being tracked. We also find that the tracking ability obtained solely from simulators can potentially transfer to real-world scenarios. We demonstrate successful examples of such transfer via experiments over the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation. End-to-end Active Object Tracking and Its Real-world Deployment via Reinforcement Learning.