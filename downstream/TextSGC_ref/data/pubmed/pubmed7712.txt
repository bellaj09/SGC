Establishing a solid theoretical foundation for structured deep neural networks is greatly desired due to the successful applications of deep learning in various practical domains. This paper aims at an approximation theory of deep convolutional neural networks whose structures are induced by convolutions. To overcome the difficulty in theoretical analysis of the networks with linearly increasing widths arising from convolutions we introduce a downsampling operator to reduce the widths. We prove that the downsampled deep convolutional neural networks can be used to approximate ridge functions nicely which hints some advantages of these structured networks in terms of approximation or modeling. We also prove that the output of any multi-layer fully-connected neural network can be realized by that of a downsampled deep convolutional neural network with free parameters of the same order which shows that in general the approximation ability of deep convolutional neural networks is at least as good as that of fully-connected networks. Finally a theorem for approximating functions on Riemannian manifolds is presented which demonstrates that deep convolutional neural networks can be used to learn manifold features of data. Theory of deep convolutional neural networks: Downsampling.