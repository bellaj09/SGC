"Although our brain and deep neural networks DNNs can perform high-level sensory-perception tasks such as image or speech recognition the inner mechanism of these hierarchical information-processing systems is poorly understood in both neuroscience and machine learning. Recently Morcos et al. 2018 examined the effect of class-selective units in DNNs i.e. units with high-level selectivity on network generalization concluding that hidden units that are selectively activated by specific input patterns may harm the networks performance. In this study we revisited their hypothesis considering units with selectivity for lower-level features and argue that selective units are not always harmful to the network performance. Specifically by using DNNs trained for image classification we analyzed the orientation selectivity of individual units a low-level selectivity widely studied in visual neuroscience. We found that orientation-selective units exist in both lower and higher layers of these DNNs as in our brain. In particular units in lower layers became more orientation-selective as the generalization performance improved during the course of training. Consistently networks that generalized better were more orientation-selective in the lower layers. We finally revealed that ablating these selective units in the lower layers substantially degraded the generalization performance of the networks at least by disrupting the shift-invariance of the higher layers. These results suggest that orientation selectivity can play a causally important role in object recognition and that contrary to the triviality of units with high-level selectivity lower-layer units with selectivity for low-level features may be indispensable for generalization at least for the several network architectures." Causal importance of low-level feature selectivity for generalization in image recognition.