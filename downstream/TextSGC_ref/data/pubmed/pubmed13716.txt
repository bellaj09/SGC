Recent advances in deep learning for medical image segmentation demonstrate expert-level accuracy. However application of these models in clinically realistic environments can result in poor generalization and decreased accuracy mainly due to the domain shift across different hospitals scanner vendors imaging protocols and patient populations etc. Common transfer learning and domain adaptation techniques are proposed to address this bottleneck. However these solutions require data and annotations from the target domain to retrain the model and is therefore restrictive in practice for widespread model deployment. Ideally we wish to have a trained locked model that can work uniformly well across unseen domains without further training. In this paper we propose a deep stacked transformation approach for domain generalization. Specifically a series of n stacked transformations are applied to each image during network training. The underlying assumption is that the "expected" domain shift for a specific medical imaging modality could be simulated by applying extensive data augmentation on a single source domain and consequently a deep model trained on the augmented "big" data BigAug could generalize well on unseen domains. We exploit four surprisingly effective but previously understudied image-based characteristics for data augmentation to overcome the domain generalization problem. We train and evaluate the BigAug model with n = 9 transformations on three different 3D segmentation tasks prostate gland left atrial left ventricle covering two medical imaging modalities MRI and ultrasound involving eight publicly available challenge datasets. The results show that when training on relatively small dataset n=10~32 volumes depending on the size of the available datasets from a single source domain: i BigAug models degrade an average of 11% Dice score change from source to unseen domain substantially better than conventional augmentation degrading 39% and CycleGAN-based domain adaptation method degrading 25% ii BigAug is better than "shallower" stacked transforms i.e. those with fewer transforms on unseen domains and demonstrates modest improvement to conventional augmentation on the source domain iii after training with BigAug on one source domain performance on an unseen domain is similar to training a model from scratch on that domain when using the same number of training samples. When training on large datasets n=465 volumes with BigAug iv application to unseen domains reaches the performance of state-of-the-art fully supervised models that are trained and tested on their source domains. These findings establish a strong benchmark for the study of domain generalization in medical imaging and can be generalized to the design of highly robust deep segmentation models for clinical deployment. Generalizing Deep Learning for Medical Image Segmentation to Unseen Domains via Deep Stacked Transformation.