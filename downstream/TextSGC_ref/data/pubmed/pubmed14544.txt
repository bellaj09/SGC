In classification tasks restricted Boltzmann machines RBMs have predominantly been used in the first stage either as feature extractors or to provide initialization of neural networks. In this study we propose a discriminative learning approach to provide a self-contained RBM method for classification inspired by free-energy based function approximation FE-RBM originally proposed for reinforcement learning. For classification the FE-RBM method computes the output for an input vector and a class vector by the negative free energy of an RBM. Learning is achieved by stochastic gradient-descent using a mean-squared error training objective. In an earlier study we demonstrated that the performance and the robustness of FE-RBM function approximation can be improved by scaling the free energy by a constant that is related to the size of network. In this study we propose that the learning performance of RBM function approximation can be further improved by computing the output by the negative expected energy EE-RBM instead of the negative free energy. To create a deep learning architecture we stack several RBMs on top of each other. We also connect the class nodes to all hidden layers to try to improve the performance even further. We validate the classification performance of EE-RBM using the MNIST data set and the NORB data set achieving competitive performance compared with other classifiers such as standard neural networks deep belief networks classification RBMs and support vector machines. The purpose of using the NORB data set is to demonstrate that EE-RBM with binary input nodes can achieve high performance in the continuous input domain. Expected energy-based restricted Boltzmann machine for classification.