To increase the ability of brain-machine interfaces BMIs to control advanced prostheses such as the modular prosthetic limb MPL we are developing a novel system: the Hybrid Augmented Reality Multimodal Operation Neural Integration Environment HARMONIE. This system utilizes hybrid input supervisory control and intelligent robotics to allow users to identify an object via eye tracking and computer vision and initiate via brain-control a semi-autonomous reach-grasp-and-drop of the object by the MPL. Sequential iterations of HARMONIE were tested in two pilot subjects implanted with electrocorticographic ECoG and depth electrodes within motor areas. The subjects performed the complex task in 71.4% 20/28 and 67.7% 21/31 of trials after minimal training. Balanced accuracy for detecting movements was 91.1% and 92.9% significantly greater than chance accuracies p < 0.05. After BMI-based initiation the MPL completed the entire task 100% one object and 70% three objects of the time. The MPL took approximately 12.2 s for task completion after system improvements implemented for the second subject. Our hybrid-BMI design prevented all but one baseline false positive from initiating the system. The novel approach demonstrated in this proof-of-principle study using hybrid input supervisory control and intelligent robotics addresses limitations of current BMIs. Demonstration of a semi-autonomous hybrid brain-machine interface using human intracranial EEG eye tracking and computer vision to control a robotic upper limb prosthetic.