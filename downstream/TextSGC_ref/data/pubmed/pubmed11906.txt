Recently deep learning based video super-resolution SR methods combine the convolutional neural networks CNN with motion compensation to estimate a high-resolution HR video from its low-resolution LR counterpart. However most previous methods conduct downscaling motion estimation to handle large motions which can lead to detrimental effects on the accuracy of motion estimation due to the reduction of spatial resolution. Besides these methods usually treat different types of intermediate features equally which lack flexibility to emphasize meaningful information for revealing the high-frequency details. In this paper to solve above issues we propose a deep dual attention network DDAN including a motion compensation network MCNet and a SR reconstruction network ReconNet to fully exploit the spatio-temporal informative features for accurate video SR. The MCNet progressively learns the optical flow representations to synthesize the motion information across adjacent frames in a pyramid fashion. To decrease the mis-registration errors caused by the optical flow based motion compensation we extract the detail components of original LR neighboring frames as complementary information for accurate feature extraction. In the ReconNet we implement dual attention mechanisms on a residual unit and form a residual attention unit to focus on the intermediate informative features for high-frequency details recovery. Extensive experimental results on numerous datasets demonstrate the proposed method can effectively achieve superior performance in terms of quantitative and qualitative assessments compared with state-of-the-art methods. Learning a Deep Dual Attention Network for Video Super-Resolution.