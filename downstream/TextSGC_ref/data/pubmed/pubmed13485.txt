Motivated by insights from the maxout-units-based deep Convolutional Neural Network CNN that "non-maximal features are unable to deliver" and "feature mapping subspace pooling is insufficient" we present a novel mixed variant of the recently introduced maxout unit called a mixout unit. Specifically we do so by calculating the exponential probabilities of feature mappings gained by applying different convolutional transformations over the same input and then calculating the expected values according to their exponential probabilities. Moreover we introduce the Bernoulli distribution to balance the maximum values with the expected values of the feature mappings subspace. Finally we design a simple model to verify the pooling ability of mixout units and a Mixout-units-based Network-in-Network NiN model to analyze the feature learning ability of the mixout models. We argue that our proposed units improve the pooling ability and that mixout models can achieve better feature learning and classification performance. Improving deep convolutional neural networks with mixed maxout units.