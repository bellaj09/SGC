Recently deep learning algorithms have outperformed human experts in various tasks across several domains; however their characteristics are distant from current knowledge of neuroscience. The simulation results of biological learning algorithms presented herein outperform state-of-the-art optimal learning curves in supervised learning of feedforward networks. The biological learning algorithms comprise asynchronous input signals with decaying input summation weights adaptation and multiple outputs for an input signal. In particular the generalization error for such biological perceptrons decreases rapidly with increasing number of examples and it is independent of the size of the input. This is achieved using either synaptic learning or solely through dendritic adaptation with a mechanism of swinging between reflecting boundaries without learning steps. The proposed biological learning algorithms outperform the optimal scaling of the learning curve in a traditional perceptron. It also results in a considerable robustness to disparity between weights of two networks with very similar outputs in biological supervised learning scenarios. The simulation results indicate the potency of neurobiological mechanisms and open opportunities for developing a superior class of deep learning algorithms. Biological learning curves outperform existing ones in artificial intelligence algorithms.