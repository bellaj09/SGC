A novel deep learning architecture was explored to create synthetic CT MRCT images that preserve soft tissue contrast necessary for support of patient positioning in Radiation therapy. A U-Net architecture was applied to learn the correspondence between input T1-weighted MRI and spatially aligned corresponding CT images. The network was trained on sagittal images taking advantage of the left-right symmetry of the brain to increase the amount of training data for similar anatomic positions. The output CT images were divided into three channels representing Hounsfield Unit HU ranges of voxels containing air soft tissue and bone respectively and simultaneously trained using a combined Mean Absolute Error MAE and Mean Squared Error MSE loss function equally weighted for each channel. Training on 9192 image pairs yielded resulting synthetic CT images on 13 test patients with MAE of 17.6+/-3.4 HU range 14-26.5 HU in soft tissue. Varying the amount of training data demonstrated a general decrease in MAE values with more data with the lack of a plateau indicating that additional training data could further improve correspondence between MRCT and CT tissue intensities. Treatment plans optimized on MRCT-derived density grids using this network for 7 radiosurgical targets had doses recalculated using the corresponding CT-derived density grids yielding a systematic mean target dose difference of 2.3% due to the lack of the immobilization mask on the MRCT images and a standard deviation of 0.1% indicating the consistency of this correctable difference. Alignment of MRCT and cone beam CT CBCT images used for patient positioning demonstrated excellent preservation of dominant soft tissue features and alignment comparisons of treatment planning CT scans to CBCT images vs. MRCT to CBCT alignment demonstrated differences of -0.1  0.2 mm -0.1  0.3 mm and -0.2  0.3 mm about the left-right anterior-posterior and cranial-caudal axes respectively. Generation of Synthetic CT Images From MRI for Treatment Planning and Patient Positioning Using a 3-Channel U-Net Trained on Sagittal Images.