The explosive growth of text data requires effective methods to represent and classify these texts. Many text learning methods have been proposed like statistics-based methods semantic similarity methods and deep learning methods. The statistics-based methods focus on comparing the substructure of text which ignores the semantic similarity between different words. Semantic similarity methods learn a text representation by training word embedding and representing text as the average vector of all words. However these methods cannot capture the topic diversity of words and texts clearly. Recently deep learning methods such as CNNs and RNNs have been studied. However the vanishing gradient problem and time complexity for parameter selection limit their applications. In this paper we propose a novel and efficient text learning framework named Latent Topic Text Representation Learning. Our method aims to provide an effective text representation and text measurement with latent topics. With the assumption that words on the same topic follow a Gaussian distribution texts are represented as a mixture of topics i.e. a Gaussian mixture model. Our framework is able to effectively measure text distance to perform text categorization tasks by leveraging statistical manifolds. Experimental results on text representation and classification and topic coherence demonstrate the effectiveness of the proposed method. Latent Topic Text Representation Learning on Statistical Manifolds.