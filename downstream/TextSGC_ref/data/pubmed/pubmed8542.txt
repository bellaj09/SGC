Learning 3D global features by aggregating multiple views is important. Pooling is widely used to aggregate views in deep learning models. However pooling disregards a lot of content information within views and the spatial relationship among the views which limits the discriminability of learned features. To resolve this issue 3D to Sequential Views 3D2SeqViews is proposed to more effectively aggregate the sequential views using convolutional neural networks with a novel hierarchical attention aggregation. Specifically the content information within each view is first encoded. Then the encoded view content information and the sequential spatiality among the views are simultaneously aggregated by the hierarchical attention aggregation where view-level attention and class-level attention are proposed to hierarchically weight sequential views and shape classes. View-level attention is learned to indicate how much attention is paid to each view by each shape class which subsequently weights sequential views through a novel recursive view integration. Recursive view integration learns the semantic meaning of view sequence which is robust to the first view position. Furthermore class-level attention is introduced to describe how much attention is paid to each shape class which innovatively employs the discriminative ability of the fine-tuned network. 3D2SeqViews learns more discriminative features than the state-of-the-art which leads to the outperforming results in shape classification and retrieval under three large-scale benchmarks. 3D2SeqViews: Aggregating Sequential Views for 3D Global Feature Learning by CNN With Hierarchical Attention Aggregation.