The choice of kernel has an important effect on the performance of a support vector machine SVM. The effect could be reduced by NEUROSVM an architecture using multilayer perceptron for feature extraction and SVM for classification. In binary classification a general linear kernel NEUROSVM can be theoretically simplified as an input layer many hidden layers and an SVM output layer. As a feature extractor the sub-network composed of the input and hidden layers is first trained together with a virtual ordinary output layer by backpropagation then with the output of its last hidden layer taken as input of the SVM classifier for further training separately. By taking the sub-network as a kernel mapping from the original input space into a feature space we present a novel model called deep neural mapping support vector machine DNMSVM from the viewpoint of deep learning. This model is also a new and general kernel learning method where the kernel mapping is indeed an explicit function expressed as a sub-network different from an implicit function induced by a kernel function traditionally. Moreover we exploit a two-stage procedure of contrastive divergence learning and gradient descent for DNMSVM to jointly training an adaptive kernel mapping instead of a kernel function without requirement of kernel tricks. As a whole of the sub-network and the SVM classifier the joint training of DNMSVM is done by using gradient descent to optimize the objective function with the sub-network layer-wise pre-trained via contrastive divergence learning of restricted Boltzmann machines. Compared to the separate training of NEUROSVM the joint training is a new algorithm for DNMSVM to have advantages over NEUROSVM. Experimental results show that DNMSVM can outperform NEUROSVM and RBFSVM i.e. SVM with the kernel of radial basis function demonstrating its effectiveness. Deep neural mapping support vector machines.