This paper describes three coarse image description strategies which are meant to promote a rough perception of surrounding objects for visually impaired individuals with application to indoor spaces. The described algorithms operate on images grabbed by the user by means of a chest-mounted camera and provide in output a list of objects that likely exist in his context across the indoor scene. In this regard first different colour texture and shape-based feature extractors are generated followed by a feature learning step by means of AutoEncoder AE models. Second the produced features are fused and fed into a multilabel classifier in order to list the potential objects. The conducted experiments point out that fusing a set of AE-learned features scores higher classification rates with respect to using the features individually. Furthermore with respect to reference works our method: i yields higher classification accuracies and ii runs at least four times faster which enables a potential full real-time application. Real-Time Indoor Scene Description for the Visually Impaired Using Autoencoder Fusion Strategies with Visible Cameras.