This paper addresses a common challenge with computational cognitive models: identifying parameter values that are both theoretically plausible and generate predictions that match well with empirical data. While computational models can offer deep explanations of cognition they are computationally complex and often out of reach of traditional parameter fitting methods. Weak methodology may lead to premature rejection of valid models or to acceptance of models that might otherwise be falsified. Mathematically robust fitting methods are therefore essential to the progress of computational modeling in cognitive science. In this article we investigate the capability and role of modern fitting methods-including Bayesian optimization and approximate Bayesian computation-and contrast them to some more commonly used methods: grid search and Nelder-Mead optimization. Our investigation consists of a reanalysis of the fitting of two previous computational models: an Adaptive Control of Thought-Rational model of skill acquisition and a computational rationality model of visual search. The results contrast the efficiency and informativeness of the methods. A key advantage of the Bayesian methods is the ability to estimate the uncertainty of fitted parameter values. We conclude that approximate Bayesian computation is a efficient b informative and c offers a path to reproducible results. Parameter Inference for Computational Cognitive Models with Approximate Bayesian Computation.