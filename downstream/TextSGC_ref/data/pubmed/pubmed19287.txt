In the field of spatial coding it is well established that we mentally represent objects for action not only relative to ourselves egocentrically but also relative to other objects landmarks allocentrically. Several factors facilitate allocentric coding for example when objects are task-relevant or constitute stable and reliable spatial configurations. What is unknown however is how object-semantics facilitate the formation of these spatial configurations and thus allocentric coding. Here we demonstrate that i we can quantify the semantic similarity of objects and that ii semantically similar objects can serve as a cluster of landmarks that are allocentrically coded. Participants arranged a set of objects based on their semantic similarity. These arrangements were then entered into a similarity analysis. Based on the results we created two semantic classes of objects natural and man-made that we used in a virtual reality experiment. Participants were asked to perform memory-guided reaching movements toward the initial position of a target object in a scene while either semantically congruent or incongruent landmarks were shifted. We found that the reaching endpoints systematically deviated in the direction of landmark shift. Importantly this effect was stronger for shifts of semantically congruent landmarks. Our findings suggest that object-semantics facilitate allocentric coding by creating stable spatial configurations. Facilitation of allocentric coding by virtue of object-semantics.