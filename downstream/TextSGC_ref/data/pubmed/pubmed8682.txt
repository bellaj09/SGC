"Video-based person re-identification is an important task with the challenges of lighting variation low-resolution images background clutter occlusion and human appearance similarity in the multi-camera visual sensor networks. In this paper we propose a video-based person re-identification method called the end-to-end learning architecture with hybrid deep appearance-temporal feature. It can learn the appearance features of pivotal frames the temporal features and the independent distance metric of different features. This architecture consists of two-stream deep feature structure and two Siamese networks. For the first-stream structure we propose the Two-branch Appearance Feature TAF sub-structure to obtain the appearance information of persons and used one of the two Siamese networks to learn the similarity of appearance features of a pairwise person. To utilize the temporal information we designed the second-stream structure that consisting of the Optical flow Temporal Feature OTF sub-structure and another Siamese network to learn the persons temporal features and the distances of pairwise features. In addition we select the pivotal frames of video as inputs to the Inception-V3 network on the Two-branch Appearance Feature sub-structure and employ the salience-learning fusion layer to fuse the learned global and local appearance features. Extensive experimental results on the PRID2011 iLIDS-VID and Motion Analysis and Re-identification Set MARS datasets showed that the respective proposed architectures reached 79% 59% and 72% at Rank-1 and had advantages over state-of-the-art algorithms. Meanwhile it also improved the feature representation ability of persons." Video-Based Person Re-Identification by an End-To-End Learning Architecture with Hybrid Deep Appearance-Temporal Feature.