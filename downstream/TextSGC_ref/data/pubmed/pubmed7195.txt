In this paper we propose a novel deep learning framework called spatial-temporal recurrent neural network STRNN to integrate the feature learning from both spatial and temporal information of signal sources into a unified spatial-temporal dependency model. In STRNN to capture those spatially co-occurrent variations of human emotions a multidirectional recurrent neural network RNN layer is employed to capture long-range contextual cues by traversing the spatial regions of each temporal slice along different directions. Then a bi-directional temporal RNN layer is further used to learn the discriminative features characterizing the temporal dependencies of the sequences where sequences are produced from the spatial RNN layer. To further select those salient regions with more discriminative ability for emotion recognition we impose sparse projection onto those hidden states of spatial and temporal domains to improve the model discriminant ability. Consequently the proposed two-layer RNN model provides an effective way to make use of both spatial and temporal dependencies of the input signals for emotion recognition. Experimental results on the public emotion datasets of electroencephalogram and facial expression demonstrate the proposed STRNN method is more competitive over those state-of-the-art methods. Spatial-Temporal Recurrent Neural Network for Emotion Recognition.