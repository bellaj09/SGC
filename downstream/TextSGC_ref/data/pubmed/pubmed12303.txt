Given a function dictionary D and an approximation budget NN nonlinear approximation seeks the linear combination of the best N terms Formula: see text to approximate a given function f with the minimum approximation error Formula: see text Motivated by recent success of deep learning we propose dictionaries with functions in a form of compositions i.e. Formula: see text for all TD and implement T using ReLU feed-forward neural networks FNNs with L hidden layers. We further quantify the improvement of the best N-term approximation rate in terms of N when L is increased from 1 to 2 or 3 to show the power of compositions. In the case when L>3 our analysis shows that increasing L cannot improve the approximation rate in terms of N. In particular for any function f on 01 regardless of its smoothness and even the continuity if f can be approximated using a dictionary when L=1 with the best N-term approximation rate Lf=ON- we show that dictionaries with L=2 can improve the best N-term approximation rate to Lf=ON-2. We also show that for Hlder continuous functions of order  on 01d the application of a dictionary with L=3 in nonlinear approximation can achieve an essentially tight best N-term approximation rate Lf=ON-2d. Finally we show that dictionaries consisting of wide FNNs with a few hidden layers are more attractive in terms of computational efficiency than dictionaries with narrow and very deep FNNs for approximating Hlder continuous functions if the number of computer cores is larger than N in parallel computing. Nonlinear approximation via compositions.