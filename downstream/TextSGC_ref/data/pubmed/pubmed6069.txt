Image segmentation is one of the most important methods for animal phenome research. Since the advent of deep learning many researchers have looked at multilayer convolutional neural networks to solve the problems of image segmentation. A network simplifies the task of image segmentation with automatic feature extraction. Many networks struggle to output accurate details when dealing with pixel-level segmentation. In this paper we propose a new concept: Depth density. Based on a depth image produced by a Kinect system we design a new function to calculate the depth density value of each pixel and bring this value back to the result of semantic segmentation for improving the accuracy. In the experiment we choose Simmental cattle as the target of image segmentation and fully convolutional networks FCN as the verification networks. We proved that depth density can improve four metrics of semantic segmentation pixel accuracy mean accuracy mean intersection over union and frequency weight intersection over union by 2.9% 0.3% 11.4% and 5.02% respectively. The result shows that depth information produced by Kinect can improve the accuracy of the semantic segmentation of FCN. This provides a new way of analyzing the phenotype information of animals. Depth Density Achieves a Better Result for Semantic Segmentation with the Kinect System.