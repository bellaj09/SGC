Different modalities such as structural MRI FDG-PET and CSF have complementary information which is likely to be very useful for diagnosis of AD and MCI. Therefore it is possible to develop a more effective and accurate AD/MCI automatic diagnosis method by integrating complementary information of different modalities. In this paper we propose multi-modal sparse hierarchical extreme leaning machine MSH-ELM. We used volume and mean intensity extracted from 93 regions of interest ROIs as features of MRI and FDG-PET respectively and used p-tau t-tau and A  42  as CSF features. In detail high-level representation was individually extracted from each of MRI FDG-PET and CSF using a stacked sparse extreme learning machine auto-encoder sELM-AE. Then another stacked sELM-AE was devised to acquire a joint hierarchical feature representation by fusing the high-level representations obtained from each modality. Finally we classified joint hierarchical feature representation using a kernel-based extreme learning machine KELM. The results of MSH-ELM were compared with those of conventional ELM single kernel support vector machine SK-SVM multiple kernel support vector machine MK-SVM and stacked auto-encoder SAE. Performance was evaluated through 10-fold cross-validation. In the classification of AD vs. HC and MCI vs. HC problem the proposed MSH-ELM method showed mean balanced accuracies of 96.10% and 86.46% respectively which is much better than those of competing methods. In summary the proposed algorithm exhibits consistently better performance than SK-SVM ELM MK-SVM and SAE in the two binary classification problems AD vs. HC and MCI vs. HC. "Identification of Alzheimers disease and mild cognitive impairment using multimodal sparse hierarchical extreme learning machine."