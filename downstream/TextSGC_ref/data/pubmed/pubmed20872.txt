As the field of brain-machine interfaces and neuro-prosthetics continues to grow there is a high need for sensor and actuation mechanisms that can provide haptic feedback to the user. Current technologies employ expensive invasive and often inefficient force feedback methods resulting in an unrealistic solution for individuals who rely on these devices. This paper responds through the development integration and analysis of a novel feedback architecture where haptic information during the neural control of a prosthetic hand is perceived through multi-frequency auditory signals. Through representing force magnitude with volume and force location with frequency the feedback architecture can translate the haptic experiences of a robotic end effector into the alternative sensory modality of sound. Previous research with the proposed cross-modal feedback method confirmed its learnability so the current work aimed to investigate which frequency map i.e. frequency-specific locations on the hand is optimal in helping users distinguish between hand-held objects and tasks associated with them. After short use with the cross-modal feedback during the electromyographic EMG control of a prosthetic hand testing results show that users are able to use audial feedback alone to discriminate between everyday objects. While users showed adaptation to three different frequency maps the simplest map containing only two frequencies was found to be the most useful in discriminating between objects. This outcome provides support for the feasibility and practicality of the cross-modal feedback method during the neural control of prosthetics. Object discrimination using optimized multi-frequency auditory cross-modal haptic feedback.