In this work travel destinations and business locations are taken as venues. Discovering a venue by a photograph is very important for visual context-aware applications. Unfortunately few efforts paid attention to complicated real images such as venue photographs generated by users. Our goal is fine-grained venue discovery from heterogeneous social multimodal data. To this end we propose a novel deep learning model category-based deep canonical correlation analysis. Given a photograph as input this model performs: 1 exact venue search find the venue where the photograph was taken and 2 group venue search find relevant venues that have the same category as the photograph by the cross-modal correlation between the input photograph and textual description of venues. In this model data in different modalities are projected to a same space via deep networks. Pairwise correlation between different modality data from the same venue for exact venue search and category-based correlation between different modality data from different venues with the same category for group venue search are jointly optimized. Because a photograph cannot fully reflect rich text description of a venue the number of photographs per venue in the training phase is increased to capture more aspects of a venue. We build a new venue-aware multimodal data set by integrating Wikipedia featured articles and Foursquare venue photographs. Experimental results on this data set confirm the feasibility of the proposed method. Moreover the evaluation over another publicly available data set confirms that the proposed method outperforms state of the arts for cross-modal retrieval between image and text. Category-Based Deep CCA for Fine-Grained Venue Discovery From Multimodal Data.