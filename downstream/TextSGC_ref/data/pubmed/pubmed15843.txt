Traditional video compression approaches build upon the hybrid coding framework with motion-compensated prediction and residual transform coding. In this paper taking advantage of both the classical compression architecture and the powerful non-linear representation ability of neural networks we propose the first end-to-end deep video compression framework. Our framework employs pixel-wise motion information which is learned from an optical flow network and further compressed by an auto-encoder network to save bits. The other compression components are also implemented by well-designed networks for high efficiency. All the modules are jointly optimized by using the rate-distortion trade-off and collaborate with each other. More importantly the proposed deep video compression framework is very flexible and can be easily extended by using lightweight or advanced networks for higher speed or better efficiency. Experimental results show that the proposed approach can outperform the widely used video coding standard H.264 and be even on par with the latest standard H.265. An End-to-End Learning Framework for Video Compression.