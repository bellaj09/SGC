Many classical Computer Vision problems such as essential matrix computation and pose estimation from 3D to 2D correspondences can be tackled by solving a linear least-square problem which can be done by finding the eigenvector corresponding to the smallest or zero eigenvalue of a matrix representing a linear system. Incorporating this in deep learning frameworks would allow us to explicitly encode known notions of geometry instead of having the network implicitly learn them from data. However performing eigendecomposition within a network requires the ability to differentiate this operation. While theoretically doable this introduces numerical instability in the optimization process in practice. In this paper we introduce an eigendecomposition-free approach to training a deep network whose loss depends on the eigenvector corresponding to a zero eigenvalue of a matrix predicted by the network. We demonstrate that our approach is much more robust than explicit differentiation of the eigendecomposition using two general tasks outlier rejection and denoising with several practical examples including wide-baseline stereo the perspective-n-point problem and ellipse fitting. Empirically our method has better convergence properties and yields state-of-the-art results. Eigendecomposition-Free Training of Deep Networks for Linear Least-Square Problems.