Spiking neural networks SNNs are believed to be highly computationally and energy efficient for specific neurochip hardware real-time solutions. However there is a lack of learning algorithms for complex SNNs with recurrent connections comparable in efficiency with back-propagation techniques and capable of unsupervised training. Here we suppose that each neuron in a biological neural network tends to maximize its activity in competition with other neurons and put this principle at the basis of a new SNN learning algorithm. In such a way a spiking network with the learned feed-forward reciprocal and intralayer inhibitory connections is introduced to the MNIST database digit recognition. It has been demonstrated that this SNN can be trained without a teacher after a short supervised initialization of weights by the same algorithm. Also it has been shown that neurons are grouped into families of hierarchical structures corresponding to different digit classes and their associations. This property is expected to be useful to reduce the number of layers in deep neural networks and modeling the formation of various functional structures in a biological nervous system. Comparison of the learning properties of the suggested algorithm with those of the Sparse Distributed Representation approach shows similarity in coding but also some advantages of the former. The basic principle of the proposed algorithm is believed to be practically applicable to the construction of much more complicated and diverse task solving SNNs. We refer to this new approach as "Family-Engaged Execution and Learning of Induced Neuron Groups" or FEELING. Recurrent Spiking Neural Network Learning Based on a Competitive Maximization of Neuronal Activity.