Brain state classification for communication and control has been well established in the area of brain-computer interfaces over the last decades. Recently the passive and automatic extraction of additional information regarding the psychological state of users from neurophysiological signals has gained increased attention in the interdisciplinary field of affective computing. We investigated how well specific emotional reactions induced by auditory stimuli can be detected in EEG recordings. We introduce an auditory emotion induction paradigm based on the International Affective Digitized Sounds 2nd Edition IADS-2 database also suitable for disabled individuals. Stimuli are grouped in three valence categories: unpleasant neutral and pleasant. Significant differences in time domain domain event-related potentials are found in the electroencephalogram EEG between unpleasant and neutral as well as pleasant and neutral conditions over midline electrodes. Time domain data were classified in three binary classification problems using a linear support vector machine SVM classifier. We discuss three classification performance measures in the context of affective computing and outline some strategies for conducting and reporting affect classification studies. EEG Responses to Auditory Stimuli for Automatic Affect Recognition.