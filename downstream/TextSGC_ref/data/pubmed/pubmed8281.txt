Epilepsy monitoring involves the study of videos to assess clinical signs semiology to assist with the diagnosis of seizures. Recent advances in the application of vision-based approaches to epilepsy analysis have demonstrated significant potential to automate this assessment. Nevertheless current proposed computer vision based techniques are unable to accurately quantify specific facial modifications e.g. mouth motions which are examined by neurologists to distinguish between seizure types. 2D approaches that analyse facial landmarks have been proposed to quantify mouth motions however they are unable to fully represent motions in the mouth and cheeks ictal pouting due to a lack of landmarks in the the cheek regions. Additionally 2D region-based techniques based on the detection of the mouth have limitations when dealing with large pose variations and thus make a fair comparison between samples difficult due to the variety of poses present. 3D approaches on the other hand retain rich information about the shape and appearance of faces simplifying alignment for comparison between sequences. In this paper we propose a novel network method based on a 3D reconstruction of the face and deep learning to detect and quantify mouth semiology in our video dataset of 20 seizures recorded from patients with mesial temporal and extra-temporal lobe epilepsy. The proposed network is capable of distinguishing between seizures of both types of epilepsy. An average classification accuracy of 89% demonstrates the benefits of computer vision and deep learning for clinical applications of non-contact systems to identify semiology commonly encountered in a natural clinical setting. Vision-Based Mouth Motion Analysis in Epilepsy: A 3D Perspective.