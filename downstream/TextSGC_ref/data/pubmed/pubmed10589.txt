We present a theoretical analysis of singular points of artificial deep neural networks resulting in providing deep neural network models having no critical points introduced by a hierarchical structure. It is considered that such deep neural network models have good nature for gradient-based optimization. First we show that there exist a large number of critical points introduced by a hierarchical structure in deep neural networks as straight lines depending on the number of hidden layers and the number of hidden neurons. Second we derive a sufficient condition for deep neural networks having no critical points introduced by a hierarchical structure which can be applied to general deep neural networks. It is also shown that the existence of critical points introduced by a hierarchical structure is determined by the rank and the regularity of weight matrices for a specific class of deep neural networks. Finally two kinds of implementation methods of the sufficient conditions to have no critical points are provided. One is a learning algorithm that can avoid critical points introduced by the hierarchical structure during learning called avoidant learning algorithm. The other is a neural network that does not have some critical points introduced by the hierarchical structure as an inherent property called avoidant neural network. Resolution of Singularities Introduced by Hierarchical Structure in Deep Neural Networks.