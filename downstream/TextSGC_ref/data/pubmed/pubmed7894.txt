Computational speech segregation attempts to automatically separate speech from noise. This is challenging in conditions with interfering talkers and low signal-to-noise ratios. Recent approaches have adopted deep neural networks and successfully demonstrated speech intelligibility improvements. A selection of components may be responsible for the success with these state-of-the-art approaches: the system architecture a time frame concatenation technique and the learning objective. The aim of this study was to explore the roles and the relative contributions of these components by measuring speech intelligibility in normal-hearing listeners. A substantial improvement of 25.4 percentage points in speech intelligibility scores was found going from a subband-based architecture in which a Gaussian Mixture Model-based classifier predicts the distributions of speech and noise for each frequency channel to a state-of-the-art deep neural network-based architecture. Another improvement of 13.9 percentage points was obtained by changing the learning objective from the ideal binary mask in which individual time-frequency units are labeled as either speech- or noise-dominated to the ideal ratio mask where the units are assigned a continuous value between zero and one. Therefore both components play significant roles and by combining them speech intelligibility improvements were obtained in a six-talker condition at a low signal-to-noise ratio. The benefit of combining a deep neural network architecture with ideal ratio mask estimation in computational speech segregation to improve speech intelligibility.