In this paper we propose YoTube-a novel deep learning framework for generating action proposals in untrimmed videos where each action proposal corresponds to a spatial-temporal tube that potentially locates one human action. Most of the existing works generate proposals by clustering low-level features or linking image proposals which ignore the interplay between long-term temporal context and short-term cues. Different from these works our method considers the interplay by designing a new recurrent YoTube detector and static YoTube detector. The recurrent YoTube detector sequentially regresses candidate bounding boxes using Recurrent Neural Network learned long-term temporal contexts. The static YoTube detector produces bounding boxes using rich appearance cues in every single frame. To fully exploit the complementary appearance motion and temporal context we train the recurrent and static detector using RGB Color and flow information. Moreover we fuse the corresponding outputs of the detectors to produce accurate and robust proposal boxes and obtain the final action proposals by linking the proposal boxes using dynamic programming with a novel path trimming method. Benefiting from the pipeline of our method the untrimmed video could be effectively and efficiently handled. Extensive experiments on the challenging UCF-101 UCF-Sports and JHMDB datasets show superior performance of the proposed method compared with the state of the arts. YoTube: Searching Action Proposal Via Recurrent and Static Regression Networks.