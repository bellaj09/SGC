Deep reinforcement learning DRL is applied to control a nonlinear chaotic system governed by the one-dimensional Kuramoto-Sivashinsky KS equation. DRL uses reinforcement learning principles for the determination of optimal control solutions and deep neural networks for approximating the value function and the control policy. Recent applications have shown that DRL may achieve superhuman performance in complex cognitive tasks. In this work we show that using restricted localized actuation partial knowledge of the state based on limited sensor measurements and model-free DRL controllers it is possible to stabilize the dynamics of the KS system around its unstable fixed solutions here considered as target states. The robustness of the controllers is tested by considering several trajectories in the phase space emanating from different initial conditions; we show that DRL is always capable of driving and stabilizing the dynamics around target states. The possibility of controlling the KS system in the chaotic regime by using a DRL strategy solely relying on local measurements suggests the extension of the application of RL methods to the control of more complex systems such as drag reduction in bluff-body wakes or the enhancement/diminution of turbulent mixing. Control of chaotic systems by deep reinforcement learning.