During human-robot collaborations HRC robot systems must accurately perceive the actions and intentions of humans. The present study proposes the classification of standing postures from standing-pressure images by which a robot system can predict the intended actions of human workers in an HRC environment. To this end it explores deep learning based on standing-posture recognition and a multi-recognition algorithm fusion method for HRC. To acquire the pressure-distribution data ten experimental participants stood on a pressure-sensing floor embedded with thin-film pressure sensors. The pressure data of nine standing postures were obtained from each participant. The human standing postures were discriminated by seven classification algorithms. The results of the best three algorithms were fused using the Dempster-Shafer evidence theory to improve the accuracy and robustness. In a cross-validation test the best method achieved an average accuracy of 99.96%. The convolutional neural network classifier and data-fusion algorithm can feasibly classify the standing postures of human workers. Standing-Posture Recognition in Human-Robot Collaboration Based on Deep Learning and the Dempster-Shafer Evidence Theory.