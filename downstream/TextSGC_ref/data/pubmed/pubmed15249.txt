Artificial neural networks ANNs are important building blocks in technical applications. They rely on noiseless continuous signals in stark contrast to the discrete action potentials stochastically exchanged among the neurons in real brains. We propose to bridge this gap with Spike-by-Spike SbS networks which represent a compromise between non-spiking and spiking versions of generative models. What is missing however are algorithms for finding weight sets that would optimize the output performances of deep SbS networks with many layers. Here a learning rule for feed-forward SbS networks is derived. The properties of this approach are investigated and its functionality is demonstrated by simulations. In particular a Deep Convolutional SbS network for classifying handwritten digits achieves a classification performance of roughly 99.3% on the MNIST test data when the learning rule is applied together with an optimizer. Thereby it approaches the benchmark results of ANNs without extensive parameter optimization. We envision this learning rule for SBS networks to provide a new basis for research in neuroscience and for technical applications especially when they become implemented on specialized computational hardware. Back-Propagation Learning in Deep Spike-By-Spike Networks.