"Although real-world environments are often multisensory visual scientists typically study visual learning in unisensory environments containing visual signals only. Here we use deep or artificial neural networks to address the question Can multisensory training aid visual learning? We examine a networks internal representations of objects based on visual signals in two conditions: a when the network is initially trained with both visual and haptic signals and b when it is initially trained with visual signals only. Our results demonstrate that a network trained in a visual-haptic environment in which visual but not haptic signals are orientation-dependent tends to learn visual representations containing useful abstractions such as the categorical structure of objects and also learns representations that are less sensitive to imaging parameters such as viewpoint or orientation that are irrelevant for object recognition or classification tasks. We conclude that researchers studying perceptual learning in vision-only contexts may be overestimating the difficulties associated with important perceptual learning problems. Although multisensory perception has its own challenges perceptual learning can become easier when it is considered in a multisensory setting." Can multisensory training aid visual learning? A computational investigation.