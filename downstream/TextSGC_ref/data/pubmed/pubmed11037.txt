We propose a deep neural network architecture the Encode-Manipulate-Decode EM*D net for rapid manipulation planning on deformable objects. We demonstrate its effectiveness on simulated cloth. The net consists of 3D convolutional encoder and decoder modules that map cloth states to and from latent space with a "manipulation module" in between that learns a forward model of the cloth\s dynamics w.r.t. the manipulation repertoire in latent space. The manipulation module\s architecture is specialized for its role as a forward model iteratively modifying a state representation by means of residual connections and repeated input at every layer. We train the network to predict the post-manipulation cloth state from a pre-manipulation cloth state and a manipulation input. By training the network end-to-end we force the encoder and decoder modules to learn a latent state representation that facilitates modification by the manipulation module. We show that the network can achieve good generalization from a training dataset of 6000 manipulation examples. Comparative experiments without the architectural specializations of the manipulation module show reduced performance confirming the benefits of our architecture. Manipulation plans are generated by performing error back-propagation w.r.t. the manipulation inputs. Recurrent use of the manipulation network during planning allows for generation of multi-step plans. We show results for plans of up to three manipulations demonstrating generally good approximation of the goal state. Plan generation takes <2.5 s for a three-step plan and is found to be robust to cloth self-occlusion supporting the approach\ viability for practical application. Fast and Flexible Multi-Step Cloth Manipulation Planning Using an Encode-Manipulate-Decode Network EM*D Net.