With the development of deep learning in medical image analysis decoding brain states from functional magnetic resonance imaging fMRI signals has made significant progress. Previous studies often utilized deep neural networks to automatically classify brain activity patterns related to diverse cognitive states. However due to the individual differences between subjects and the variation in acquisition parameters across devices the inconsistency in data distributions degrades the performance of cross-subject decoding. Besides most current networks were trained in a supervised way which is not suitable for the actual scenarios in which massive amounts of data are unlabeled. To address these problems we proposed the deep crosssubject adaptation decoding DCAD framework to decipher the brain states. The proposed volume-based 3D feature extraction architecture can automatically learn the common spatiotemporal features of labeled source data to generate a distinct descriptor. Then the distance between the source and target distributions is minimized via an unsupervised domain adaptation UDA method which can help to accurately decode the cognitive states across subjects. The performance of the DCAD was evaluated on task-fMRI tfMRI dataset from the Human Connectome Project HCP. Experimental results showed that the proposed method achieved the state-of-the-art decoding performance with mean 81.9% and 84.9% accuracies under two conditions 4 brain states and 9 brain states respectively of working memory task. Our findings also demonstrated that UDA can mitigate the impact of the data distribution shift thereby providing a superior choice for increasing the performance of cross-subject decoding without depending on annotations. Decoding Brain States from fMRI Signals by using Unsupervised Domain Adaptation.