Discovering the visual features and representations used by the brain to recognize objects is a central problem in the study of vision. Recently neural network models of visual object recognition including biological and deep network models have shown remarkable progress and have begun to rival human performance in some challenging tasks. These models are trained on image examples and learn to extract features and representations and to use them for categorization. It remains unclear however whether the representations and learning processes discovered by current models are similar to those used by the human visual system. Here we show by introducing and using minimal recognizable images that the human visual system uses features and processes that are not used by current models and that are critical for recognition. We found by psychophysical studies that at the level of minimal recognizable images a minute change in the image can have a drastic effect on recognition thus identifying features that are critical for the task. Simulations then showed that current models cannot explain this sensitivity to precise feature configurations and more generally do not learn to recognize minimal images at a human level. The role of the features shown here is revealed uniquely at the minimal level where the contribution of each feature is essential. A full understanding of the learning and use of such features will extend our understanding of visual recognition and its cortical mechanisms and will enhance the capacity of computational models to learn from visual experience and to deal with recognition and detailed image interpretation. Atoms of recognition in human and computer vision.