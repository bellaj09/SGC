As hybrid passive brain-computer interface systems become more advanced it is important to grow our understanding of how to produce generalizable pattern classifiers of physiological data. One of the most difficult problems in applying machine learning algorithms to these data types is nonstationarity which can evolve over the course of hours and days and is more susceptible to changes resulting from complex cognitive function in comparison to simple stimulus-based processes. This nonstationarity referenced as day-to-day variability results in the inability of many learning algorithms to generalize to new data. In previous work we have shown that increasing the number of unique testing sessions used to form a learning set can improve the accuracy of classifying mental workload in a binary state paradigm. While this result was very promising we did not address whether the additional discriminability was the result of a larger learning set or the uniqueness contributed by the testing sessions being spread over multiple days. Further the simulation task used in this prior analysis was low-fidelity with respect to the task it attempted to model; whether these methods extend to more realistic task simulation environments has not been comparatively investigated. In this work we compare these previous results to a second study with a similar multi-day paradigm that required participants to perform a more realistic simulation task. Comparative analysis of these two studies reveals that the improved generalization of the multi-day learning set is attributable in large part to the uniqueness of the multi-day paradigm. Further this multi-day effect was also observed in the higher fidelity simulation study. These results help to validate the use of the multi-day learning set approach for improving overall system classification accuracy. Future studies should consider the use of multi-day designs for improving generalizability over other interesting dimensions. Day-to-day variability in hybrid passive brain-computer interfaces: comparing two studies assessing cognitive workload.