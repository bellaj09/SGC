Several theories propose that the cortex implements an internal model to explain predict and learn about sensory data but the nature of this model is unclear. One condition that could be highly informative here is Charles Bonnet syndrome CBS where loss of vision leads to complex vivid visual hallucinations of objects people and whole scenes. CBS could be taken as indication that there is a generative model in the brain specifically one that can synthesise rich consistent visual representations even in the absence of actual visual input. The processes that lead to CBS are poorly understood. Here we argue that a model recently introduced in machine learning the deep Boltzmann machine DBM could capture the relevant aspects of hypothetical generative processing in the cortex. The DBM carries both the semantics of a probabilistic generative model and of a neural network. The latter allows us to model a concrete neural mechanism that could underlie CBS namely homeostatic regulation of neuronal activity. We show that homeostatic plasticity could serve to make the learnt internal model robust against e.g. degradation of sensory input but overcompensate in the case of CBS leading to hallucinations. We demonstrate how a wide range of features of CBS can be explained in the model and suggest a potential role for the neuromodulator acetylcholine. This work constitutes the first concrete computational model of CBS and the first application of the DBM as a model in computational neuroscience. Our results lend further credence to the hypothesis of a generative model in the brain. Charles Bonnet syndrome: evidence for a generative model in the cortex?