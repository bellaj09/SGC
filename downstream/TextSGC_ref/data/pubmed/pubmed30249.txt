Percepts and words can be decoded from distributed neural activity measures. However the existence of widespread representations might conflict with the more classical notions of hierarchical processing and efficient coding which are especially relevant in speech processing. Using fMRI and magnetoencephalography during syllable identification we show that sensory and decisional activity colocalize to a restricted part of the posterior superior temporal gyrus pSTG. Next using intracortical recordings we demonstrate that early and focal neural activity in this region distinguishes correct from incorrect decisions and can be machine-decoded to classify syllables. Crucially significant machine decoding was possible from neuronal activity sampled across different regions of the temporal and frontal lobes despite weak or absent sensory or decision-related responses. These findings show that speech-sound categorization relies on an efficient readout of focal pSTG neural activity while more distributed activity patterns although classifiable by machine learning instead reflect collateral processes of sensory perception and decision. Focal versus distributed temporal cortex activity for speech sound category assignment.