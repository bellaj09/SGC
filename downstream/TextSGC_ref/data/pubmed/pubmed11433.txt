For supervised speech enhancement contextual information is important for accurate mask estimation or spectral mapping. However commonly used deep neural networks DNNs are limited in capturing temporal contexts. To leverage long-term contexts for tracking a target speaker we treat speech enhancement as a sequence-to-sequence mapping and present a novel convolutional neural network CNN architecture for monaural speech enhancement. The key idea is to systematically aggregate contexts through dilated convolutions which significantly expand receptive fields. The CNN model additionally incorporates gating mechanisms and residual learning. Our experimental results suggest that the proposed model generalizes well to untrained noises and untrained speakers. It consistently outperforms a DNN a unidirectional long short-term memory LSTM model and a bidirectional LSTM model in terms of objective speech intelligibility and quality metrics. Moreover the proposed model has far fewer parameters than DNN and LSTM models. Gated Residual Networks with Dilated Convolutions for Monaural Speech Enhancement.