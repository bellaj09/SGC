Although deep neural networks DNNs have led to many remarkable results in cognitive tasks they are still far from catching up with human-level cognition in antinoise capability. New research indicates how brittle and susceptible current models are to small variations in data distribution. In this letter we study the stochasticity-resistance character of biological neurons by simulating the input-output response process of a leaky integrate-and-fire LIF neuron model and proposed a novel activation function rand softplus RSP to model the response process. In RSP a scale factor <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi></mml:mi></mml:math> is employed to mimic the stochasticity-adaptability of biological neurons thereby enabling the antinoise capability of a DNN to be improved by the novel activation function. We validated the performance of RSP with a 19-layer residual network ResNet and a 19-layer visual geometry group VGG on facial expression recognition data sets and compared it with other popular activation functions such as rectified linear units ReLU softplus leaky ReLU LReLU exponential linear unit ELU and noisy softplus NSP. The experimental results show that RSP is applied to VGG-19 or ResNet-19 and the average recognition accuracy under five different noise levels exceeds the other functions on both of the two facial expression data sets; in other words RSP outperforms the other activation functions in noise resistance. Compared with the application in ResNet-19 the application of RSP in VGG-19 can improve a network\s antinoise performance to a greater extent. In addition RSP is easier to train compared to NSP because it has only one parameter to be calculated automatically according to the input data. Therefore this work provides the deep learning community with a novel activation function that can better deal with overfitting problems. Improving the Antinoise Ability of DNNs via a Bio-Inspired Noise Adaptive Activation Function Rand Softplus.