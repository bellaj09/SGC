Deep learning models have gained great success in many real-world applications. However most existing networks are typically designed in heuristic manners thus these approaches lack of rigorous mathematical derivations and clear interpretations. Several recent studies try to build deep models by unrolling a particular optimization model that involves task information. Unfortunately due to the dynamic nature of network parameters their resultant deep propagations do not possess the nice convergence property as the original optimization scheme does. In this work we develop a generic paradigm to unroll nonconvex optimization for deep model design. Different from most existing frameworks which just replace the iterations by network architectures we prove in theory that the propagation generated by our proximally unrolled deep model can globally converge to the critical-point of the original optimization model. Moreover even if the task information is only partially available e.g. no prior regularization we can still train a convergent deep propagations. We also extend these theoretical investigations on the more general multi-block models and thus a lot of real-world applications can be successfully handled by the proposed framework. Finally we conduct experiments on various low-level vision tasks i.e. non-blind deconvolution dehazing and low-light image enhancement and demonstrate the superiority of our proposed framework compared with existing state-of-the-art approaches. Deep Proximal Unrolling: Algorithmic Framework Convergence Analysis and Applications.