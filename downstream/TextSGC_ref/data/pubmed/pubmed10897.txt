Human action Recognition for unknown views is a challenging task. We propose a deep view-invariant human action recognition framework which is a novel integration of two important action cues: motion and shape temporal dynamics STD. The motion stream encapsulates the motion content of action as RGB Dynamic Images RGB-DIs which are generated by Approximate Rank Pooling ARP and processed by using finetuned InceptionV3 model. The STD stream learns long-term view-invariant shape dynamics of action using a sequence of LSTM and Bi-LSTM learning models. Human Pose Model HPM generates view-invariant features of structural similarity index matrix SSIM based key depth human pose frames. The final prediction of the action is made on the basis of three types of late fusion techniques i.e. maximum max average avg and multiply mul applied on individual stream scores. To validate the performance of the proposed novel framework the experiments are performed using both cross-subject and cross-view validation schemes on three publically available benchmarks- NUCLA multi-view dataset UWA3D-II Activity dataset and NTU RGB-D Activity dataset. Our algorithm outperforms existing state-of-the-arts significantly which is measured in terms of recognition accuracy receiver operating characteristic ROC curve and area under the curve AUC. View-invariant Deep Architecture for Human Action Recognition using Two-stream Motion and Shape Temporal Dynamics.