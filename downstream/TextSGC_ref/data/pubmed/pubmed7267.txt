Deep convolutional neural networks CNNs have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers both of which are performed on local image areas without considering the dependence among different image regions. However such dependence is very important for generating explicit image representation. In contrast recurrent neural networks RNNs are well known for their ability of encoding contextual information in sequential data and they only require a limited number of network parameters. Thus we proposed the hierarchical RNNs HRNNs to encode the contextual dependence in image representation. In HRNNs each RNN layer focuses on modeling spatial dependence among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically we propose two RNN models: 1 hierarchical simple recurrent network HSRN which is fast and has low computational cost and 2 hierarchical long-short term memory recurrent network which performs better than HSRN with the price of higher computational cost. In this paper we integrate CNNs with HRNNs and develop end-to-end convolutional hierarchical RNNs C-HRNNs for image classification. C-HRNNs not only utilize the discriminative representation power of CNNs but also utilize the contextual dependence learning ability of our HRNNs. On four of the most challenging object/scene image classification benchmarks our C-HRNNs achieve the state-of-the-art results on Places 205 SUN 397 and MIT indoor and the competitive results on ILSVRC 2012. Learning Contextual Dependence With Convolutional Hierarchical Recurrent Neural Networks.