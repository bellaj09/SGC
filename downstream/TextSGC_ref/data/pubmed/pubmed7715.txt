Enabling deep neural networks for tight resource constraint environments like mobile phones and cameras is the current need. The existing availability in the form of optimized architectures like Squeeze Net MobileNet etc. are devised to serve the purpose by utilizing the parameter friendly operations and architectures such as point-wise convolution bottleneck layer etc. This work focuses on optimizing the number of floating point operations involved in inference through an already compressed deep learning architecture. The optimization is performed by utilizing the advantage of residual connections in a macroscopic way. This paper proposes novel connection on top of the deep learning architecture whose idea is to locate the blocks of a pretrained network which have relatively lesser knowledge quotient and then bypassing those blocks by an intelligent skip connection named here as Shunt connection. The proposed method helps in replacing the high computational blocks by computation friendly shunt connection. In a given architecture up to two vulnerable locations are selected where 6 contiguous blocks are selected and skipped at the first location and 2 contiguous blocks are selected and skipped at the second location leveraging 2 shunt connections. The proposed connection is used over state-of-the-art MobileNet-V2 architecture and manifests two cases which lead from 33.5% reduction in flops one connection up to 43.6% reduction in flops two connections with minimal impact on accuracy. Shunt connection: An intelligent skipping of contiguous blocks for optimizing MobileNet-V2.