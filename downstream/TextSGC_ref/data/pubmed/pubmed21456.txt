For people who cannot communicate due to severe paralysis or involuntary movements technology that decodes intended speech from the brain may offer an alternative means of communication. If decoding proves to be feasible intracranial Brain-Computer Interface systems can be developed which are designed to translate decoded speech into computer generated speech or to instructions for controlling assistive devices. Recent advances suggest that such decoding may be feasible from sensorimotor cortex but it is not clear how this challenge can be approached best. One approach is to identify and discriminate elements of spoken language such as phonemes. We investigated feasibility of decoding four spoken phonemes from the sensorimotor face area using electrocorticographic signals obtained with high-density electrode grids. Several decoding algorithms including spatiotemporal matched filters spatial matched filters and support vector machines were compared. Phonemes could be classified correctly at a level of over 75% with spatiotemporal matched filters. Support Vector machine analysis reached a similar level but spatial matched filters yielded significantly lower scores. The most informative electrodes were clustered along the central sulcus. Highest scores were achieved from time windows centered around voice onset time but a 500\xa0ms window before onset time could also be classified significantly. The results suggest that phoneme production involves a sequence of robust and reproducible activity patterns on the cortical surface. Importantly decoding requires inclusion of temporal information to capture the rapid shifts of robust patterns associated with articulator muscle group contraction during production of a phoneme. The high classification scores are likely to be enabled by the use of high density grids and by the use of discrete phonemes. Implications for use in Brain-Computer Interfaces are discussed. Decoding spoken phonemes from sensorimotor cortex with high-density ECoG grids.