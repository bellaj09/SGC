Visual SLAM is one of the key technologies to align the virtual and real world together in Augmented Reality applications. RGBD dense Visual SLAM approaches have shown their advantages in robustness and accuracy in recent years. However there are still several challenges such as the inconsistencies in RGBD measurements across multiple frames that could jeopardize the accuracy of both camera trajectory and scene reconstruction. In this paper we propose a novel map representation called Probabilistic Surfel Map PSM for dense visual SLAM. The main idea is to maintain a globally consistent map with both photometric and geometric uncertainties encoded in order to address the inconsistency issue. The key of our PSM is proper modeling and updating of sensor measurement uncertainties as well as the strategies to apply them for improving both the front-end pose estimation and the back-end optimization. Experimental results on publicly available datasets demonstrate major improvements with our approach over the state-of-the-art methods. Specifically comparing with -DVO we achieve a 40% reduction in absolute trajectory error and an 18% reduction in relative pose error in visual odometry as well as an 8.5% reduction in absolute trajectory error in complete SLAM. Moreover our PSM enables generation of a high quality dense point cloud with comparable accuracy as the state-of-the-art approach. Dense Visual SLAM with Probabilistic Surfel Map.