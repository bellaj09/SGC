Increasingly autonomous agents will be required to operate on long-term missions. This will create a demand for general intelligence because feedback from a human operator may be sparse and delayed and because not all behaviours can be prescribed. Deep neural networks and reinforcement learning methods can be applied in such environments but their fixed updating routines imply an inductive bias in learning spatio-temporal patterns meaning some environments will be unsolvable. To address this problem this paper proposes active adaptive perception the ability of an architecture to learn when and how to modify and selectively utilise its perception module. To achieve this a generic architecture based on a self-modifying policy SMP is proposed and implemented using Incremental Self-improvement with the Success Story Algorithm. The architecture contrasts to deep reinforcement learning systems which follow fixed training strategies and earlier SMP studies which for perception relied either entirely on the working memory or on untrainable active perception instructions. One computationally cheap and one more expensive implementation are presented and compared to DRQN an off-policy deep reinforcement learner using experience replay and Incremental Self-improvement an SMP on various non-episodic partially observable mazes. The results show that the simple instruction set leads to emergent strategies to avoid detracting corridors and rooms and that the expensive implementation allows selectively ignoring perception where it is inaccurate. Learning to learn with active adaptive perception.