"We present a deep learning strategy to fuse multiple semantic cues for complex event recognition. In particular we tackle the recognition task by answering how to jointly analyze human actions who is doing what objects what and scenes where. First each type of semantic features e.g. human action trajectories is fed into a corresponding multi-layer feature abstraction pathway followed by a fusion layer connecting all the different pathways. Second the correlations of how the semantic cues interacting with each other are learned in an unsupervised cross-modality autoencoder fashion. Finally by fine-tuning a large-margin objective deployed on this deep architecture we are able to answer the question on how the semantic cues of who what and where compose a complex event. As compared with the traditional feature fusion methods e.g. various early or late strategies our method jointly learns the essential higher level features that are most effective for fusion and recognition. We perform extensive experiments on two real-world complex event video benchmarks MED11 and CCV and demonstrate that our method outperforms the best published results by 21% and 11% respectively on an event recognition task." Deep Fusion of Multiple Semantic Cues for Complex Event Recognition.