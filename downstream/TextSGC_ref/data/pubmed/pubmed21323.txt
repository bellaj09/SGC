From the desktop to the laptop to the mobile device personal computing platforms evolve over time. Moving forward wearable computing is widely expected to be integral to consumer electronics and beyond. The primary interface between a wearable computer and a user is often a near-eye display. However current generation near-eye displays suffer from multiple limitations: they are unable to provide fully natural visual cues and comfortable viewing experiences for all users. At their core many of the issues with near-eye displays are caused by limitations in conventional optics. Current displays cannot reproduce the changes in focus that accompany natural vision and they cannot support users with uncorrected refractive errors. With two prototype near-eye displays we show how these issues can be overcome using display modes that adapt to the user via computational optics. By using focus-tunable lenses mechanically actuated displays and mobile gaze-tracking technology these displays can be tailored to correct common refractive errors and provide natural focus cues by dynamically updating the system based on where a user looks in a virtual scene. Indeed the opportunities afforded by recent advances in computational optics open up the possibility of creating a computing platform in which some users may experience better quality vision in the virtual world than in the real one. Optimizing virtual reality for all users through gaze-contingent and adaptive focus displays.